{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project - The Battle of Neighborhoods \n",
    "## 'The Best Walkable Spots for Students in NY'\n",
    "\n",
    "### Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this first week, you have to clearly define a problem or an idea of your choice, where you would need to leverage the Foursquare location data to solve or execute. Remember that data science problems always target an audience and are meant to help a group of stakeholders solve a problem, so make sure that you explicitly describe your audience and why they would care about your problem.\n",
    "\n",
    "This submission will eventually become your Introduction/Business Problem section in your final report. So I recommend that you push the report (having your Introduction/Business Problem section only for now) to your Github repository and submit a link to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "__1.Introduction - Business Problem Definition.__\n",
    "\n",
    "__2. Data__\n",
    "\n",
    "__3. Methodology__\n",
    "\n",
    "__4. Results__\n",
    "\n",
    "__5. Discussion__\n",
    "\n",
    "__6. Conclusion__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction - Business Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Study Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There a category of students or young adults that would like to settle temporarily in New York City. This type of people has often low financial resources. They cannot afford to own and maintain a car with all the expenses that it can generate. Sometimes it is also because they want to adopt a greener transportation style to reduce their carbon footprint.\n",
    "\n",
    "To help for their daily expenses or to supplement their basic income, they often hold jobs in the restaurants, shops, bars, etc. Moreover, as generally with young people, they also like to frequent the lively neighborhoods to meet their friends or other people. \n",
    "\n",
    "We can therefore predict that this category of population will be interested in looking for rental housing in lively areas allowing both leisure outings and opportunities to find odd jobs.\n",
    "\n",
    "Of course, since they do not have a personal vehicle because lacking financial means or by the choice of ecological convictions, these students or young people will certainly look for renting nearby rooms or studios in neighborhoods as close as possible to places where they can easily find extra work.\n",
    "\n",
    "A criterion of choice will be the proximity of the place of residence to these employment zones, having the possibility of getting there as quickly as possible first by walking as a pedestrian then using a skate-board, a bicycle or by public transport such as bus lines or metro etc ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As a Data Scientist, I would like to study the neighborhoods of New York and make a classification according to the number of places of shopping, outings and leisure and the accessibility of these places by walking (in a first study).\n",
    "\n",
    "Later, we can refine by considering also other transportation means such as bike or public transport such as buses and metro.\n",
    "\n",
    "Finally to be closer to the reality of these low-income people, we can add the rental price criterion (average in the neighborhood) to study the influence on the classification.\n",
    "\n",
    "Nota: in this case study, the important factor of \"income required per tenant\" to be able to rent a room has not not been considered. Of course, it must be taken into account in a real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Audience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who could be interested in this problem ?\n",
    "\n",
    "At least we can list the following target populations :\n",
    "\n",
    "1. Students that would like to find a job close to the place of residence\n",
    "\n",
    "2. People who do not own any personal vehicle because they have either ecological convictions or low income but who love lively places with lots of restaurants, cafes, shops, cultural places etc. So those who are interested in living in a place with a high level walk-ability index.\n",
    "\n",
    "3. Companies that work around search engines for rental accommodation agencies\n",
    "\n",
    "4. Reception centers and help for students "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of needed data can be established from the problem definition above. \n",
    "\n",
    "As an output of the solution, we would like to provide a global picture of the New York places indicating where it is possible to rent a room or studio and with a score calculated based on the proximity of restaurants, bars, Coffee Shops or other businesses fully accessible to pedestrians. Something like a heat-map of places in New York City.\n",
    "\n",
    "So we will need to retrieve the following information:\n",
    "\n",
    "1. The list and localisation of New York neigborhoods\n",
    "2. The list and localisation of places such as coffee shop, restaurants, miscellaneous shop, theaters etc around a specific place in New York \n",
    "3. The list of places with their walkability measure\n",
    "4. The list of places with their average rental price\n",
    "\n",
    "In the next sections, each data set is described given the origin, the type of data and the way it will be used to solve the problem we defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1. New York neigborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2. Venues in New York Neigborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Walkability Measures of New York Places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Average Rental Price of New York Places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization and Dependencies\n",
    "Before we get the data and start exploring it, let's download all the dependencies that we will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # library to handle data in a vectorized manner\n",
    "\n",
    "import pandas as pd # library for data analsysis\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import json # library to handle JSON files\n",
    "import os\n",
    "import time\n",
    "\n",
    " # uncomment this line if you haven't completed the Foursquare API lab\n",
    "#!conda install -c conda-forge geopy --yes\n",
    "# Use pip instead\n",
    "#!pip3 install geopy\n",
    "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
    "\n",
    "import requests # library to handle requests\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "#import pandas.api.CategoricalDtype\n",
    "#from pandas.api.types import CategoricalDtype\n",
    "\n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "# import k-means from clustering stage\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "#µ!pip3 install folium0.5.0\n",
    "import folium # map rendering library\n",
    "\n",
    "# Install website scraping libraries and packages in Python from BeautifulSoup \n",
    "#!conda install -c conda-forge beautifulsoup4 --yes  # uncomment this line if you haven't completed \n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utility data and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neede for filtering venues retrieval in Foursquare\n",
    "target_root_categories = ['4d4b7105d754a06374d81259','4d4b7104d754a06370d81259',\n",
    "                          '4d4b7105d754a06373d81259','4d4b7105d754a06376d81259',\n",
    "                          '4d4b7105d754a06377d81259','4d4b7105d754a06378d81259'\n",
    "                         ]\n",
    "\n",
    "# Check if a file is existing\n",
    "def not_present(filename):\n",
    "    return not os.path.isfile('./'+ filename)\n",
    "\n",
    "\n",
    "# Arrays to define values and order for Categorical columns \n",
    "ws_cat = ['Car-Dependant', 'Somewhat Walkable', 'Very Walkable', 'Walker’s Paradise']\n",
    "budget_cat =['Cheap', 'Average', 'Pricey']\n",
    "\n",
    "# Function to convert string array into categorical array\n",
    "# Return a value to setup order for a categorical variable\n",
    "def set_col_cat(df, catTab=budget_cat):\n",
    "    raw_cat = pd.Categorical(df, categories=catTab, ordered=True)\n",
    "    df = raw_cat\n",
    "    return df\n",
    " \n",
    "# Return a value to setup order for a categorical variable\n",
    "def convertCatToNum(x,cat_type):\n",
    "    val = None \n",
    "    if (cat_type == 'b'):  # budget category\n",
    "        if x == 'Cheap':\n",
    "            val = 2        # 1\n",
    "        elif x == 'Average':\n",
    "            val = 1     # 0\n",
    "    elif cat_type == 'w':   # Walk Score category\n",
    "        if x == 'Very Walkable':\n",
    "            val = 1      #0\n",
    "        elif x == 'Walker’s Paradise':\n",
    "            val = 2      #1\n",
    "    else:\n",
    "        print('convertCatToNum : Invalid cat_type parameter', cat_type)\n",
    "    \n",
    "    return val\n",
    "\n",
    "# Re-interpret/Rescale Budget label relative to price intervals\n",
    "def rescaleBudgetLabel(br1):\n",
    "    if(br1 >= 0 and br1 <= 2149):\n",
    "        cat = 0\n",
    "    elif (br1 <= 3149):\n",
    "        cat = 1\n",
    "    else :\n",
    "        cat = 2\n",
    "    return budget_cat[cat]\n",
    " \n",
    "#ws_cat = ['Car-Dependant', 'Somewhat Walkable', 'Very Walkable', 'Walker’s Paradise'] \n",
    "# Re-interpret/Rescale label relative to Walk score intervals\n",
    "def getWalkLabel(ws):\n",
    "    cat = -1\n",
    "    if(ws >= 0 and ws <= 49):\n",
    "        cat = 0\n",
    "    elif (ws <= 69):\n",
    "        cat = 1\n",
    "    elif (ws <= 89):\n",
    "        cat = 2\n",
    "    elif (ws <= 100):\n",
    "        cat = 3\n",
    "    else :\n",
    "        return 'Unknown'\n",
    "\n",
    "    return ws_cat[cat]\n",
    "\n",
    "#p Calculate the sum of the row values \n",
    "def calc_total(row, ncol, dd):\n",
    "    dd['Count'] = 1\n",
    "    rowind = row.name\n",
    "    for i in range(len(ws_cat)):\n",
    "        val = round(dd.groupby(['Budget']).get_group(rowind).groupby(['WS_descr'])[ncol].sum()[i])\n",
    "        row[i] = val\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. New York City Neighborhoods Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New York Neighborhoods\n",
    "\n",
    "We know from a previous lab that New York city has a total of 5 boroughs and 306 neighborhoods. In order to segment the neighborhoods and explore them, we will essentially need the dataset mentioned above that contains the 5 boroughs and the neighborhoods that exist in each borough as well as the the latitude and longitude coordinates of each neighborhood.\n",
    "\n",
    "This dataset exists at the link we have been given : https://geo.nyu.edu/catalog/nyu_2451_34572. \n",
    "\n",
    "To download the description file in json format, we have to run a wget command and access the data.\n",
    "\n",
    "After that, we are able to parse the json features array of 306 items to extract all the information about the neighborhoods of New York.\n",
    "\n",
    "This step leads us to build a dataframe, neighborhoods, containing the elementary data needed to solve our problem i.e. neighborhoods with location coordinates.\n",
    "\n",
    "Using pandas library, we map the json data to the neighborhoods dataframe containing 4 columns listed below :\n",
    "\n",
    "        Borough, Neighborhood, Latitude, Longitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's go ahead and do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!wget -q -O 'newyork_data.json' https://cocl.us/new_york_dataset\n",
    "print('Data downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('newyork_data.json') as json_data: \n",
    "    newyork_data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the data.\n",
    "Display the 5 first elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of interesting data : ', newyork_data['totalFeatures'])\n",
    "{k: newyork_data[k] for k in list(newyork_data)[:2]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how all the relevant data is in the *features* key, which is basically a list of the neighborhoods. So, let's define a new variable that includes this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_data = newyork_data['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first item in this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tranform the data into a *pandas* dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is essentially transforming this data of nested Python dictionaries into a *pandas* dataframe. So let's start by creating an empty dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataframe columns\n",
    "column_names = ['Borough', 'Neighborhood', 'Latitude', 'Longitude'] \n",
    "\n",
    "# instantiate the dataframe\n",
    "neighborhoods = pd.DataFrame(columns=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the empty dataframe to confirm that the columns are as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's loop through the data and fill the dataframe one row at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in neighborhoods_data:\n",
    "    borough = data['properties']['borough'] \n",
    "    neighborhood_name = data['properties']['name']\n",
    "        \n",
    "    neighborhood_latlon = data['geometry']['coordinates']\n",
    "    neighborhood_lat = neighborhood_latlon[1]\n",
    "    neighborhood_lon = neighborhood_latlon[0]\n",
    "    \n",
    "    neighborhoods = neighborhoods.append({'Borough': borough,\n",
    "                                          'Neighborhood': neighborhood_name,\n",
    "                                          'Latitude': neighborhood_lat,\n",
    "                                          'Longitude': neighborhood_lon}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quickly examine the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"neighborhoods.shape = \", neighborhoods.shape)\n",
    "#neighborhoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And make sure that the dataset has all 5 boroughs and 306 neighborhoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataframe has {} boroughs and {} neighborhoods.'.format(\n",
    "        len(neighborhoods['Borough'].unique()),\n",
    "        neighborhoods.shape[0]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also make sure that each neighborhood is uniquely defined : there are neighborhoods names duplicated as seen below ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The dataframe has {} boroughs, {} neighborhoods but {} unique neighborhoods names.'.format(\n",
    "        len(neighborhoods['Borough'].unique()),\n",
    "        neighborhoods.shape[0],\n",
    "        len(neighborhoods['Neighborhood'].unique())\n",
    "    )\n",
    ")\n",
    "print ('Number of neighborhood name duplicates =', \n",
    "      len(neighborhoods['Neighborhood']) - len(neighborhoods['Neighborhood'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will need to rename those names to be sure that each neighborhood is uniquely defined !\n",
    "\n",
    "There are neighborhoods names duplicated as seen below ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup = neighborhoods[neighborhoods.duplicated(['Neighborhood'], keep=False)]\n",
    "dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, rename those names by adding the 2 first letters of Borough to the neighborhood name\n",
    "See below ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dup\n",
    "d_idx = df.index.values\n",
    "print(d_idx)\n",
    "\n",
    "for i, n_i in enumerate(d_idx):\n",
    "    #print('i =', i, 'n_i', n_i, ' ->orig = ', neighborhoods.iloc[n_i,0], neighborhoods.iloc[n_i,1])\n",
    "    n_name =  df.iloc[i,1]\n",
    "    b_name = (str(df.iloc[i,0]))[0:3]\n",
    "    neighborhoods.iloc[n_i,1] = n_name + ', ' +  b_name\n",
    "    print(n_name, \"---> renamed in : \", neighborhoods.iloc[n_i,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can consider the all 306 neighborhoods and try to get the most walkable areas in each of them relative to the higher density of target venues. \n",
    "1. get the area with the highest degree of venues (from fourthsquare)\n",
    "2. measure walkability for each area\n",
    "3. cluster according to walkability of each area\n",
    "\n",
    "First Let's get the geographical coordinates of New York City so that we can create a map to visually locate neighborhoods in New York City.\n",
    "\n",
    "For that we will use the _**geopy**_ library to get the coordinates and _**folium**_ library to build and display the map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use geopy library to get the latitude and longitude values of New York City."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to define an instance of the geocoder, we need to define a user_agent. We will name our agent <em>ny_explorer</em>, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'New York City, NY'\n",
    "geo_agent = \"ny_explorer\"\n",
    "\n",
    "geolocator = Nominatim(user_agent=geo_agent)\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of {} are : {}, {}.'.format(address, latitude, longitude))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a map of New York City with neighborhoods superimposed on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create map of New York using latitude and longitude values\n",
    "map_newyork = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "# add markers to map\n",
    "for lat, lng, borough, neighborhood in zip(neighborhoods['Latitude'], neighborhoods['Longitude'], neighborhoods['Borough'], neighborhoods['Neighborhood']):\n",
    "    label = '{}, {}'.format(neighborhood, borough)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color='blue',\n",
    "        fill=True,\n",
    "        fill_color='#3186cc',\n",
    "        fill_opacity=0.7,\n",
    "        parse_html=False).add_to(map_newyork)  \n",
    "    \n",
    "map_newyork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Folium** is a great visualization library. Feel free to zoom into the above map, and click on each circle mark to reveal the name of the neighborhood and its respective borough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that it is possible to locate the neighborhoods in New York City, we will explore each of them to compute the number of venues around them so that we can rank each neighborhood on a scale that measures the degree of opportunity (maximum of venues) to find a student job.\n",
    "\n",
    "To explore the venues for each neighborhood we will use the _**Foursquare**_ API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Foursquare Credentials and Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildCreds():\n",
    "    # Manage credentials\n",
    "    FSQ_CLIENT_ID = 'XXXXXXXXX' \n",
    "    FSQ_CLIENT_SECRET = 'YYYYYYYYY'\n",
    "    WS_APIKEY1 = 'ZZZZZZZZ' # Paul\n",
    "    WS_APIKEY2 = 'ZZZZZZZZ' # fafa\n",
    "    WS_APIKEY3 = 'ZZZZZZZZ' # tofrou@yopmail.com \n",
    "    WS_APIKEY4 = 'ZZZZZZZZ' # mailboxy.fun\n",
    "\n",
    "    cred_list =[FSQ_CLIENT_ID, FSQ_CLIENT_SECRET, WS_APIKEY1, WS_APIKEY2, WS_APIKEY3, WS_APIKEY4]\n",
    "    cred_cols =['API_KEY']\n",
    "    cred_df = pd.DataFrame([item for item in cred_list])\n",
    "    cred_df.columns = cred_cols\n",
    "    \n",
    "    cred_df.to_csv('Credential_apis.csv')\n",
    "\n",
    "    print ('Data file Credentials Saved !')\n",
    "\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load from local file system in case we did this before\n",
    "fromFile = False\n",
    "try:\n",
    "    cred_df = pd.read_csv('Credential_apis.csv')\n",
    "    cred_df.drop(cred_df.columns[[0]], axis=1, inplace=True)\n",
    "    print('Credentials for apis data loaded.')\n",
    "    fromFile = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# If not already loaded use the Foursquare API to get the data\n",
    "if not fromFile:\n",
    "    print ('Data file Credentials not existing! Build it')\n",
    "    cred_df = buildCreds()\n",
    "    \n",
    "for i in range(cred_df.shape[0]):\n",
    "    print ('i = ',i, ' -> key = ', 'got it')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = cred_df.iat[0, 0]     # your Foursquare ID\n",
    "CLIENT_SECRET = cred_df.iat[1, 0] # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "#LIMIT = 30\n",
    "print('Your credentails:')\n",
    "print('CLIENT_ID: ' + 'got it!')\n",
    "print('CLIENT_SECRET:' + 'got it!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to start utilizing the Foursquare API to explore the neighborhoods and segment them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each neighborhood in our dataframe, we will get the number of venues in a radius of     0.25 miles (~402 m).\n",
    "This value of the radius has been chosen because we will consider the _\"walkability\"_ score of the neighborhood that will be computed later by _**Walk Score API**_ is based on this number.\n",
    "\n",
    "As explained on the _**Walk Score**_ Web page :\n",
    "\n",
    "\" _**Walk Score Methodology**_ _*measures the walkability of any address using a patented system. For each address, Walk Score analyzes hundreds of walking routes to nearby amenities. Points are awarded based on the distance to amenities in each category. Amenities within a 5 minute walk (.25 miles) are given maximum points. A decay function is used to give points to more distant amenities, with no points given after a 30 minute walk.*_\"\n",
    "\n",
    "Of course we could have fragmented or paved the neighborhoods with a set of figures like 'circle, square, rectangle etc ..', get the center of each and calculate the number of venues.\n",
    "\n",
    "Note  : Important information : we are limited in the number of API calls for my \"Sandbox Account\" that is Sandbox Account\n",
    "\n",
    "    950 Regular Calls / Day\n",
    "    50 Premium Calls / Day\n",
    "    1 Photo per Venue\n",
    "    1 Tip per Venue Hourly Rate Limit Overview\n",
    "    An application can make a maximum of 5,000 userless requests per hour to venues/* endpoints.\n",
    "\n",
    "An application can make a maximum of 5,000 userless requests per hour to venues/* endpoints.\n",
    "So I have to take care about which information and number of calls I can perform to get what I need.\n",
    "It is why I will try to get only the venues that could interest a student i.e. those that are 'good' candidate places to find a job. Most of the data on foursquare is food (restaurants) and nightlife relate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's get the top 100 venues that are in each neighborhood within a radius of 400 meters that is less than 5 minute walk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Foursquare Exploration of Venues  in New York City Neighborhoods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create a function to get the venues information for all neighborhoods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNearbyVenues(names, latitudes, longitudes, categories=\"\", radius=500):\n",
    "    \n",
    "    if categories == \"\":\n",
    "        url_str = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'       \n",
    "    else :\n",
    "        url_str = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&categoryId={}&radius={}&limit={}'\n",
    "\n",
    "    venues_list=[]\n",
    "    i = 0\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "    #    print(i+1, \" -> \", name)\n",
    "            \n",
    "        # create the API request URL\n",
    "        if categories == \"\":\n",
    "            url = url_str.format(CLIENT_ID, CLIENT_SECRET, VERSION, \n",
    "                                 lat, lng,\n",
    "                                 radius, LIMIT)\n",
    "        else :\n",
    "            url = url_str.format(CLIENT_ID, CLIENT_SECRET, VERSION, \n",
    "                                 lat, lng, categories,\n",
    "                                 radius, LIMIT)\n",
    "\n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]\n",
    "        #print (\"response ->\", results)\n",
    "        results = results['groups'][0]\n",
    "        #print (\"groups[0] ->\", results)\n",
    "        results = results['items']\n",
    "        print(i, \" -> \", name, \" -> venues number = \", len(results))\n",
    "        i = i +1\n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(\n",
    "            name, \n",
    "            lat, \n",
    "            lng, \n",
    "            v['venue']['name'], \n",
    "            v['venue']['location']['lat'], \n",
    "            v['venue']['location']['lng'],  \n",
    "            v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_venues.columns = ['Neighborhood',\n",
    "                             'Neighborhood Latitude',\n",
    "                             'Neighborhood Longitude',\n",
    "                             'Venue',\n",
    "                             'Venue Latitude',\n",
    "                             'Venue Longitude',\n",
    "                             'Venue Category']\n",
    "    \n",
    "    return(nearby_venues)\n",
    "\n",
    "print (\"getNearbyVenues function defined !!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the code to run the above function on each neighborhood and create a new dataframe called *newyork_venues*.\n",
    "\n",
    "As we are limited by API calls, we will try to run the code once and store the result in a local file so that we can always continue the study without this limitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 500\n",
    "LIMIT = 100\n",
    "\n",
    "# Try to load from local file system in case we did this before\n",
    "fromFile = False\n",
    "try:\n",
    "    newyork_venues = pd.read_csv('NYC_Neighborhoods_venues.csv')\n",
    "    newyork_venues.drop(newyork_venues.columns[[0]], axis=1, inplace=True)\n",
    "    print('NYC Neighborhoods Venues data loaded.')\n",
    "    fromFile = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# If not already loaded use the Foursquare API to get the data\n",
    "if not fromFile:\n",
    "    print ('Data file for venues not existing ! Call API')\n",
    "    newyork_venues = getNearbyVenues(names=neighborhoods['Neighborhood'],\n",
    "                                     latitudes=neighborhoods['Latitude'],\n",
    "                                     longitudes=neighborhoods['Longitude']\n",
    "                                     )\n",
    "    newyork_venues.to_csv('NYC_Neighborhoods_venues.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine the retrieved venues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "print ('Number of retrieved venues is : ', newyork_venues.shape[0])\n",
    "newyork_venues.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10243 retrieved venues !\n",
    "\n",
    "Well seems good !\n",
    "We can see that most part of the venues are good candidates to places where a student can find a job but some of them like \"Trail\" need to be filtered out and removed from the final result.\n",
    "For the moment we let them ...\n",
    "\n",
    "Let's see the number of venues grouped by neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = newyork_venues.groupby('Neighborhood').count() #, as_index=True\n",
    "df.sort_values('Venue',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} uniques categories.'.format(len(newyork_venues['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's fisrt filter with the target venues root categories [OK,NOK]\n",
    "From the following list of root categories :\n",
    "\n",
    "#### Use API to Search for 50 top venues filtered by category type :\n",
    "\n",
    " . Arts & Entertainment : 4d4b7104d754a06370d81259            -> _**OK**_\n",
    " \n",
    " . College & University : 4d4b7105d754a06372d81259            -> _**NOK**_\n",
    "\n",
    " . Event : 4d4b7105d754a06373d81259                           -> _**OK**_\n",
    "\n",
    " . Food : 4d4b7105d754a06374d81259                            -> _**OK**_\n",
    "\n",
    " . Nightlife Spot : 4d4b7105d754a06376d81259                  -> _**OK**_\n",
    " \n",
    " . Outdoors & Recreation : 4d4b7105d754a06377d81259           -> _**OK**_ \n",
    " \n",
    " . Professional & Other Places : 4d4b7105d754a06375d81259     -> _**NOK**_\n",
    "\n",
    " . Residence : 4e67e38e036454776db1fb3a                       -> _**NOK**_\n",
    " \n",
    " . Shop & Service : 4d4b7105d754a06378d81259                  -> _**OK**_\n",
    " \n",
    " . Travel & Transport : 4d4b7105d754a06379d81259              -> _**NOK**_\n",
    " \n",
    " Of course we could discuss about this choice, given that sometimes sub-categories are or are not relevant even in not chosen root categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to format venues address\n",
    "# Lot of problems with missing fields ex: 'address' or 'city'\n",
    "# -> so take formattedAddress\n",
    "def get_address(flocation):\n",
    "    # print ('location(address) = ', location['formattedAddress']) #['address'], location['city'])\n",
    "    address = flocation[0] + ', ' + flocation[1]\n",
    "    return address\n",
    "\n",
    "# build the list of chosen categories (easier to find a job)\n",
    "root_categories = target_root_categories[0]\n",
    "for cat in  target_root_categories[1:]:\n",
    "    root_categories = root_categories + ',' + cat\n",
    "\n",
    "# build the dataframe of selected categories of venues\n",
    "# Normally, we are limited by the API to 50 retrieved venuees\n",
    "def getNearbyFilteredVenues(names, latitudes, longitudes, categories=\"\", radius=250, limit=50):\n",
    "    # Use search API with the list of selected root categories\n",
    "    url_str = \"https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&v={}&intent=browse&categoryId={}&ll={},{}&radius={}&limit={}\"\n",
    "\n",
    "    # Retrieve list of target venues in a radius of 500 m from the coordinates\n",
    "    # of each of the 306 identified neighborhoods and complying the given categories\n",
    "    venues_list=[]\n",
    "    i = 0\n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "        #print(i, \" -> \", name)\n",
    "        time.sleep(0.3)\n",
    "        url = url_str.format(CLIENT_ID, CLIENT_SECRET, VERSION,\n",
    "                             categories,lat, lng, \n",
    "                             radius, 50)\n",
    "        try:\n",
    "            results = requests.get(url).json()[\"response\"][\"venues\"]\n",
    "        except:\n",
    "            print(i, \" -> \", name, \" -> No venues !!!!\")\n",
    "            continue\n",
    "            pass\n",
    "\n",
    "        print(i, \" -> \", name, \" -> venues number = \", len(results))\n",
    "\n",
    "        # For each resulting venue list for this neighborhood\n",
    "        # build the associated data : name address coordinates etc. \n",
    "        for v in results:\n",
    "            #parsed_venue = json.loads(venue)\n",
    "            #print(\"->\",json.dumps(v, indent=4, sort_keys=True))\n",
    "            #\n",
    "            venues_list.append([(\n",
    "                name,                       # neighborhood name\n",
    "                lat,                        # neighborhood latitude\n",
    "                lng,                        # neighborhood longitude\n",
    "                v['name'],                  # venue name\n",
    "                get_address(v['location']['formattedAddress']), # venue address\n",
    "                v['location']['lat'],       # venue latitude\n",
    "                v['location']['lng'],       # venue longitude\n",
    "                v['categories'][0]['name']  # venue category\n",
    "            )])\n",
    "    \n",
    "        i = i +1\n",
    "\n",
    "    # Once we have registered all retrieved venues, put them in a new filtered dataframe\n",
    "    nearby_filtered_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_filtered_venues.columns = ['Neighborhood',\n",
    "                                      'Neighborhood Latitude',\n",
    "                                      'Neighborhood Longitude',\n",
    "                                      'Venue',\n",
    "                                      'Address',\n",
    "                                      'Venue Latitude',\n",
    "                                      'Venue Longitude',\n",
    "                                      'Venue Category']\n",
    "\n",
    "    return nearby_filtered_venues\n",
    "\n",
    "print(\"Filtered Venues Functions defined !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load from local file system in case we did this before\n",
    "fromFile = False\n",
    "try:\n",
    "    newyork_filtered_venues = pd.read_csv('NYC_Neighborhoods_filtered_venues.csv')\n",
    "    newyork_filtered_venues.drop(newyork_filtered_venues.columns[[0]], axis=1, inplace=True)\n",
    "    print('NYC Neighborhoods Filtered Venues data loaded.')\n",
    "    fromFile = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# If not already loaded use the Foursquare API to get the data\n",
    "if not fromFile:\n",
    "    print ('Data file for filtered venues not existing ! Call API')\n",
    "    newyork_filtered_venues = getNearbyFilteredVenues(names=neighborhoods['Neighborhood'],\n",
    "                                                      latitudes=neighborhoods['Latitude'],\n",
    "                                                      longitudes=neighborhoods['Longitude'],\n",
    "                                                      categories=root_categories,\n",
    "                                                      radius=500, \n",
    "                                                      limit=50\n",
    "                                                      )\n",
    "    # Export the dataframe values to a local file  \n",
    "    newyork_filtered_venues.to_csv('NYC_Neighborhoods_filtered_venues.csv')\n",
    "\n",
    "print ('Number of retrieved filtered venues is : ', newyork_filtered_venues.shape[0])\n",
    "newyork_filtered_venues[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 11311 retrieved venues (previously it was 13653 ???) !\n",
    "\n",
    "Well it seems that i got more filtered venues that for the first call without filtered request !\n",
    "In fact, each API call retrieves a different number of venues because updated. \n",
    "So let's take this number for now.\n",
    "Let's see the number of venues grouped by neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = newyork_filtered_venues.groupby('Neighborhood', as_index=False).count() #\n",
    "df = df.sort_values('Venue',ascending=False).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} uniques filtered categories.'.format(len(newyork_filtered_venues['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see how many Neighborhoods cumulate the maximum number of 5000 allowed per day by Walk API to compute the walkability of places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Venues CumSum'] = df['Venue'].cumsum()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that the first 100 elements in range [0, 99] (Lenox Hill -> Bay Terrace, Sta) cumulate from 0 to 5000 venues.\n",
    "\n",
    "Neighborhoods in range [100, 225]  (Canarsie -> Roosevelt Island) cumulate from 5001 to 9986 venues\n",
    "\n",
    "Last range [226,305] (Morris Park-> Jamaica Estates) cumulate from 9987 to 1131 venues\n",
    "\n",
    "Most part of neighborhoods cumulate more than 50 retrieved venues but limited to 50 cause API limitation.\n",
    "\n",
    "**98 \t\tCarroll Gardens \t50 \t50 \t50 \t50 \t50 \t50 \t50 \t4950 **\n",
    "\n",
    "**99 \t\tBay Terrace, Sta \t50 \t50 \t50 \t50 \t50 \t50 \t50 \t5000 **\n",
    "\n",
    "**100 \t\tCanarsie \t\t\t50 \t50 \t50 \t50 \t50 \t50 \t50 \t5050 **\n",
    "\n",
    "We can now compute the walkability for these neighborhoods in this order i.e.\n",
    "\n",
    "- loop on neighborhoods of NYC\n",
    "   - loop on each venue\n",
    "     - compute walkability \n",
    "     - add walk/bike/transit score values in new columns of newyork_filtered_venues\n",
    "   - compute mean values by neighborhood if needed\n",
    "  \n",
    "Discussion :\n",
    " - we can perform the call for each of the 11311 retrieved venues (need at leat 3 days)\n",
    " - we can just perform the walkability calculation on the first 5000 venues but 143 neighborhoods have 50 venues\n",
    " - we can pick a number of venues according to the density of retrieved venues for each neighborhood so that\n",
    "   all neighborhoods are covered and can be compared in term of walkability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Walkability Scores of New York Venues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk Scores\n",
    "\n",
    "Now that we have our location candidates, let's use Walk Score API to get info on venues in each neighborhood.\n",
    "\n",
    "So it is now possible to rank each venue, but limit is 5,000 calls by day for a Free Version. So as I have to choose the targets carefully : \n",
    " - because not sure that just take the neighborhoods with the most venues is the best strategy ??? \n",
    " - Normally should consider the \"best venues\" to find a job across all neighborhood  -> so filter by venue type ???\n",
    "\n",
    "Add each row in a new dataframe until 5000 venues is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Walk Score API format\n",
    "\n",
    "A call to the Walk Score API follows the format below :\n",
    "\n",
    "> `http://api.walkscore.com/score?format=json&address=`**ADDRESS**&lat=**LATITUDE**&lon=**LONGITUDE**&transit=1&bike=1&wsapikey=**WSAPIKEY**\n",
    "\n",
    "Just take a look at the existing data to see how to add the walking scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newyork_filtered_venues.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Walk Score GET request URL\n",
    "\n",
    "Following the format given above, thanks to the function below, I built the url from the parameters identifying a venue.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(address, lat, lon, wsapikey=\"\"):\n",
    "    url = 'http://api.walkscore.com/score?format=json&address={}&lat={}&lon={}&transit=1&bike=1&wsapikey={}'.format(\n",
    "            address, \n",
    "            lat,\n",
    "            lon,\n",
    "            wsapikey)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Walk Score Retrieval Functions Definition\n",
    "\n",
    "The functions below retrieve the walk scores and build the new dataframe with the added scores for each venue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the score of venues.\n",
    "# Normally, we are limited by the API to 5000 calls per day\n",
    "# An example of the results from WS API that we have to process\n",
    "result_str = '{\\\n",
    "    \"bike\": {\\\n",
    "        \"description\": \"Bikeable\",\\\n",
    "        \"score\": 54\\\n",
    "    },\\\n",
    "    \"description\": \"Somewhat Walkable\",\\\n",
    "    \"status\": 1,\\\n",
    "    \"walkscore\": 59\\\n",
    "}'\n",
    "\n",
    "\n",
    "def getScore(v_name, lat, lon, adr):\n",
    "    url = get_url(adr, lat, lon, wsapikey=WSAPIKEY)\n",
    "    scores_descs = [-1, \"\", -1, \"\", -1, \"\"]\n",
    "    results = requests.get(url).json() # json.loads(result_str) # \n",
    "    status = results['status']\n",
    "    \n",
    "    bscore = tscore = -1\n",
    "    bdescr = tdescr = \"\"\n",
    "    if (status != 1):\n",
    "        print(\"Venue :\", v_name, \" -> Error status returned = \", status)\n",
    "    else :\n",
    "        #print(\"->\",json.dumps(results, indent=4, sort_keys=True))\n",
    "        wscore, wdescr = results['walkscore'],results['description']\n",
    "        try:\n",
    "            bscore, bdescr = results['bike']['score'], results['bike']['description']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            tscore, tdescr = results['transit']['score'], results['transit']['description']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        scores_descs = [wscore, wdescr, bscore, bdescr, tscore, tdescr]\n",
    "\n",
    "    return scores_descs\n",
    "\n",
    "# Neighborhood Latitude \tNeighborhood Longitude \tVenue \tVenue Latitude \tVenue Longitude \tVenue Category\n",
    "def getWalkScoreVenue(venues_score_df,beg=0):\n",
    "    # Retrieve list of target venues in a radius of 500 m from the coordinates\n",
    "    # of each of the 306 identified neighborhoods and compute walk score\n",
    "    \n",
    "    venues_df_ncol = len(venues_score_df.columns)\n",
    "    for index, row in venues_score_df.iterrows():\n",
    "        name = row['Venue']\n",
    "        lat = row['Venue Latitude']\n",
    "        lng = row['Venue Longitude']\n",
    "        adr = row['Address']\n",
    "        \n",
    "        #time.sleep(0.001)\n",
    "        results = getScore(name, lat, lng, adr)\n",
    "        i = 0\n",
    "        col= venues_df_ncol - 6\n",
    "        print('index = ', index, ' Venue =', name, 'col =',col, 'results ->', results)\n",
    "        r_num = index - beg\n",
    "        venues_score_df.iat[r_num, col] = results[i] # wscore\n",
    "        i += 1\n",
    "        col +=1 \n",
    "        venues_score_df.iat[r_num, col] = results[i] # wsdescr\n",
    "        i += 1\n",
    "        col +=1 \n",
    "        venues_score_df.iat[r_num, col] = results[i]  # bscore\n",
    "        i += 1\n",
    "        col +=1 \n",
    "        venues_score_df.iat[r_num, col] = results[i] # bdescr\n",
    "        i += 1\n",
    "        col +=1 \n",
    "        venues_score_df.iat[r_num, col] = results[i] # tscore\n",
    "        i += 1\n",
    "        col +=1 \n",
    "        venues_score_df.iat[r_num, col] = results[i] # tdescr\n",
    "#    return venues_score_df\n",
    "\n",
    "# Try to load from local file system in case we did this before\n",
    "def load_scores(df_venues, beg, end):\n",
    "    filename = 'NYC_venues_walkability_{}_{}.csv'.format(beg,end)\n",
    "    fromFile = False\n",
    "    try:\n",
    "        df_venues = pd.read_csv(filename)\n",
    "        df_venues.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "        print('NYC Neighborhoods Venues Walkability Scores data loaded from file :', \n",
    "              filename)\n",
    "        df_venues.Walk_descr = set_col_cat(df_venues.Walk_descr, ws_cat)\n",
    "        fromFile = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # If not already loaded : use the Foursquare API to get the data\n",
    "    if not fromFile:\n",
    "        print ('Data file for filtered venues not existing ! Call API')\n",
    "        # Add scores columns for walkability and bike\n",
    "        df_venues = pd.concat([df_venues,\n",
    "                        pd.DataFrame(columns=['Walk_score', 'Walk_descr',\n",
    "                                              'Bike_score', 'Bike_descr',\n",
    "                                              'Transit_score', 'Transit_descr'])],\n",
    "                       sort=False)\n",
    "        getWalkScoreVenue(df_venues,beg)\n",
    "        df_venues.Walk_descr = set_col_cat(df_venues.Walk_descr, ws_cat)\n",
    "\n",
    "        # Export the dataframe values to a local file  \n",
    "        #print(df)\n",
    "        df_venues.to_csv(filename)\n",
    "        print ('Venues with Walk Scores Data file created : ', filename)\n",
    "    return df_venues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Walk Scores Retrieval\n",
    "\n",
    "Now, let's create the new dataframe from the venues extended with walk, bike, transit scores.\n",
    "\n",
    "It was difficult to do all retrievals in one operation cause the limit imposed by the Walk Score APIs (less than 5000 calls per days).\n",
    "So that I have to wait the next day to get the 5000 scores for the next 5000 venues.\n",
    "\n",
    "Also sometimes, the Python machine stops without any feedback perhaps due to the allocated machine's capacity exceeded.\n",
    "When that happens, the operation must be performed again from the beginning.\n",
    "\n",
    "It is why, I splitted the calls in several steps to get the scores for all venues the 11311 venues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "WSAPIKEY1 = cred_df.iat[i, 0] # Paul\n",
    "i=i+1\n",
    "WSAPIKEY2 = cred_df.iat[i, 0] # fafa\n",
    "i=i+1\n",
    "WSAPIKEY3 = cred_df.iat[i, 0] # tofrou@yopmail.com\n",
    "i=i+1\n",
    "WSAPIKEY4 = cred_df.iat[i, 0] # mailboxy.fun -> OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. Data Processing : First Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Data Merging\n",
    "\n",
    "As already said, I have splitted the calls in several steps to get the scores for all venues the 11311 venues.\n",
    "\n",
    "I have now to merge all dataframes and files into one unique dataframe and file to consider the totality of the information on the venues for our analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try load from existing files and concatenate the 3 tables if any\n",
    "try:\n",
    "    df_ws_0_4999 = pd.read_csv('NYC_venues_walkability_0_4999.csv')\n",
    "    df_ws_5000_9999 = pd.read_csv('NYC_venues_walkability_5000_9999.csv')\n",
    "    df_ws_10000_plus =  pd.read_csv('NYC_venues_walkability_10000_plus.csv')\n",
    "    df_venues_with_wscores = (df_0_4999.append(df_5000_9999)).append(df_ws_10000_plus)\n",
    "    df_venues_with_wscores.drop(df_venues_with_wscores.columns[[0]], axis=1, inplace=True) \n",
    "    print('NYC Neighborhoods with all Venues Walkability Scores data loaded from basic files !')\n",
    "except:\n",
    "    print('Could not load NYC Neighborhoods with all Venues Walkability Scores from basic files !')\n",
    "    pass    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load walk scores from local file system in case we already created it in a previous step\n",
    "fromFile = False\n",
    "try:\n",
    "    df_venues_with_wscores = pd.read_csv('NYC_venues_walkability_0_11310.csv')\n",
    "    df_venues_with_wscores.drop(df_venues_with_wscores.columns[[0]], axis=1, inplace=True)\n",
    "    print('NYC Neighborhoods with all Venues Walkability Scores data loaded.')\n",
    "    fromFile = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# If not already loaded redo all work at steps above (using Walk Score API to get the data)\n",
    "if not fromFile:\n",
    "    print ('Data file of Venues with Walk Scores not existing ! Back Call API')\n",
    "\n",
    "print ('Number of retrieved filtered venues is : ', df_venues_with_wscores.shape[0])\n",
    "df_venues_with_wscores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Data Cleaning\n",
    "\n",
    "Now take a look at the data and see if all information are ready to be used for our analysis !!!!\n",
    "\n",
    "Clean if any missing data : adopt a strategy !\n",
    " - drop the row\n",
    " - replace the value by the same value of the surrounding venue with the same latitude and longitude\n",
    " - replace the value by the min of values of the surrounding venues\n",
    " - replace the value by the mean of values of the surroundingvenues \n",
    " - etc.\n",
    "\n",
    "So how many venues have empty walk scores (equal to -1) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_empty_wscores = df_venues_with_wscores.index[df_venues_with_wscores['Walk_score'] == -1].tolist()\n",
    "print('Number of rows with empty Walk Score (=-1) = ', len(list_empty_wscores), \n",
    "      '\\nlist = ', list_empty_wscores, '\\n********\\n') #, df_venues_with_wscores.loc[df_venues_with_wscores['Walk_score'] == -1]['Venue'])\n",
    "for index_venue in list_empty_wscores:\n",
    "    lat = df_venues_with_wscores.loc[index_venue]['Venue Latitude']\n",
    "    lng = df_venues_with_wscores.loc[index_venue]['Venue Longitude']\n",
    "    name = df_venues_with_wscores.loc[index_venue]['Venue']\n",
    "    print (\"Index = {0:5d} at coordinates : {1:3.15f}, {2:3.15f} -> Venue : {3}\".format(index_venue, lat, lng, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the number and identification of the venues without walk score : 32 venues without walk scores.\n",
    "\n",
    "This number represents only 32/11311 = 0,000972505 ~ 0.1%\n",
    "\n",
    "So I think, it does not matter if this venues are removed before analysis.\n",
    "\n",
    "So let's drop these rows and we should get a new dataframe with (11311 - 32 =) 11279 rows.\n",
    "\n",
    "Quick check they have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venues_cleaned_wscores = df_venues_with_wscores.drop(list_empty_wscores)\n",
    "df_venues_cleaned_wscores.reset_index(drop=True, inplace=True)\n",
    "print (df_venues_cleaned_wscores.shape[0])\n",
    "\n",
    "for index_venue in list_empty_wscores[0:5]:\n",
    "    name = df_venues_cleaned_wscores.loc[index_venue]['Venue']\n",
    "    print (\"Index = {0:5d} -> Venue : {1}\".format(index_venue, name))\n",
    "\n",
    "df_venues_cleaned_wscores.to_csv(\"NYC_venues_cleaned.csv\")\n",
    "print (\"NYC_venues_cleaned.csv created !\")\n",
    "#print(df_venues_cleaned_wscores.head(), df_venues_cleaned_wscores.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load walk scores from local file system in case we already created it in a previous step\n",
    "fromFile = False\n",
    "try:\n",
    "    df_venues_with_wscores = pd.read_csv('NYC_venues_walkability_0_11310.csv')\n",
    "    df_venues_with_wscores.drop(df_venues_with_wscores.columns[[0]], axis=1, inplace=True)\n",
    "    print('NYC Neighborhoods with all Venues Walkability Scores data loaded.')\n",
    "    fromFile = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# If not already loaded redo all work at steps above (using Walk Score API to get the data)\n",
    "if not fromFile:\n",
    "    print ('Data file of Venues with Walk Scores not existing ! Back Call API')\n",
    "\n",
    "print ('Number of retrieved filtered venues is : ', df_venues_with_wscores.shape[0])\n",
    "df_venues_with_wscores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Aggregation of Average Rental Price Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already said in the Data section, these measures will help to refine choice of places in New York neighborhoods that are the most suitable for people with low incomes such as the students population.\n",
    " \n",
    "We will use the Web Site _**\"renthop\"**_ (https://www.renthop.com/) to get the page _https://www.renthop.com/average-rent-in/new-york-city-ny_ where are displayed the _**\"Rental Stats and Trends\"**_. This is a collection of information that gives:\n",
    "\n",
    "1. *Historical Prices and Trends* : curves of the rental prices of housing by categories (studios, 1 bedroom or 2) since 3 years\n",
    "\n",
    "2. *Median Rents* : table showing prices broken by housing category and quartiles (bottom 25%, median and top 25%)\n",
    "\n",
    "3. *Average Rents by Neighborhoods* : table of average prices broken by \"Neighborhoods\" and given, by housing categories, a price and a classification of the needed budget (Cheap, Average, Pricey).\n",
    "\n",
    "We are mainly interested by the data stored into the table *Average Rents by Neighborhoods*.\n",
    "\n",
    "To extract the average prices by neighborhoods and by housing categories, I will follow steps explained below :\n",
    "\n",
    "1. Download the HTML file at the given link : _'https://www.renthop.com/average-rent-in/new-york-city-ny'_ \n",
    "\n",
    "2. Register the file locally in _'NYC_renthop.html'_ \n",
    "\n",
    "3. Open the file and using _'BeautifulSoup'_ library, retrieve :\n",
    "> - the table with the title as displayed above *\"Average Rents by Neighborhoods\"*\n",
    "> - iterate through the next HTML elements to extract the name of the columns \n",
    "> - iterate through the next HTML elements to extract the values for each 'Neighborhood' row \n",
    "\n",
    "4. Build the _pandas Dataframe_ with the retrieved data\n",
    "\n",
    "Now lets do what we have presented above : parse the HTML file thanks to the _'BeautifulSoup'_ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q -O 'NYC_renthop.html' 'https://www.renthop.com/average-rent-in/new-york-city-ny'\n",
    "print('HTML NYC Renthop page downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HTML content\n",
    "with open(\"NYC_renthop.html\") as fp:\n",
    "    soup = bs(fp, 'lxml')\n",
    "\n",
    "# Get the HTML table codes\n",
    "#tagTable = soup.find('table', attrs={})\n",
    "\n",
    "allTables = soup.findAll('table')\n",
    "#print (len(allTables))\n",
    "\n",
    "avgTable = allTables[1]\n",
    "\n",
    "#Get table body for the 'Average Rents by Neighborhoods' table\n",
    "bodyAvg = avgTable.thead\n",
    "print('thead ===', bodyAvg)\n",
    "\n",
    "print('\\n***********************\\n')\n",
    "# Get 'Median Rents' data table\n",
    "medianTable = allTables[0]\n",
    "\n",
    "#Get table body for the 'Median Rents' table\n",
    "bodyMedian = medianTable.thead\n",
    "print('thead ===', bodyMedian)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tranform the data into a *pandas* dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is essentially transforming this HTML data  into a *pandas* dataframe.\n",
    "So let's start by creating an empty dataframe with just the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframe columns \n",
    "# get table column names -> all 'th' tags of the body in 'tr' fields\n",
    "def getColNames(body, n):\n",
    "    colTab = body.find_all('th')\n",
    "#print (colTab)\n",
    "    colNames = [(bs(str(colTab[i]),\"lxml\")).find('th').string.strip() for i in range(n)]\n",
    "    print(colNames)\n",
    "    return colNames\n",
    "\n",
    "avgColNames = getColNames(bodyAvg,5)\n",
    "medianColNames = getColNames(bodyMedian,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's loop through the data and fill the dataframe one row at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all 'tr' tagged fields to get table contents for each neighborhood\n",
    "rentTab= avgTable.find_all('tr')\n",
    "print(rentTab[0:2])\n",
    "print('\\n***********************\\n')\n",
    "\n",
    "medianTab = medianTable.find_all('tr')[2:]\n",
    "print(medianTab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rental amounts strings have to be formatted in numbers like below :\n",
    "\n",
    "**'$4,542 -> 4542'**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertCurrency(curStr):\n",
    "    val = -1\n",
    "    if (curStr is not None):\n",
    "        val = int(curStr.replace('$','').replace(',',''))\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the dataframe with retrieved column names\n",
    "renthop_df = pd.DataFrame(columns=avgColNames)\n",
    "\n",
    "for n, rent in enumerate(rentTab):\n",
    "    # Average Rents for the n.th Neighborhood name or link \n",
    "    # do \n",
    "    # for each element code \n",
    "    tabTd = rent.find_all('td')\n",
    "    neighborhood = tabTd[0].string # first process neighborhood\n",
    "    budget = tabTd[4].string # first process neighborhood\n",
    "\n",
    "    # for all amounts \n",
    "    tabc = [None] * 3\n",
    "    for i, value in enumerate(tabTd[1:4]):\n",
    "         tabc[i] = convertCurrency(value.string)\n",
    "#       #print('i = ', i , value.string)\n",
    "\n",
    "    price_studio = tabc[0]\n",
    "    price_1BR = tabc[1]\n",
    "    price_2BR = tabc[2]\n",
    "    \n",
    "    # insert the built postal code into the dataframe Neighborhood \tStudio \t \t2BR \tBudget\n",
    "    renthop_df = renthop_df.append({'Neighborhood' : neighborhood,\n",
    "                            'Studio' : price_studio,\n",
    "                            '1BR': price_1BR,\n",
    "                            '2BR': price_2BR,\n",
    "                            'Budget': budget},\n",
    "                           ignore_index=True)\n",
    "\n",
    "# Combine rows with same postal code into one row with the neighborhoods separated with a comma \n",
    "#df = postcode_df.groupby('Postcode', as_index=False).agg({'Borough':'first', 'Neighbourhood':', '.join})\n",
    "#renthop_df.loc[0,'Budget']\n",
    "renthop_df.Budget = set_col_cat(renthop_df.Budget)\n",
    "renthop_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_cat = renthop_df['Budget'].unique()\n",
    "print('There are {} unique Budget categories.\\n  -> {}'.format(len(budget_cat), budget_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the dataframe with retrieved column names\n",
    "medianRent_df = pd.DataFrame(columns=medianColNames)\n",
    "\n",
    "for n, mrent in enumerate(medianTab):\n",
    "    # Average Rents for the n.th Neighborhood name or link \n",
    "    # do \n",
    "    # for each element code \n",
    "    tabTd = mrent.find_all('td')\n",
    "    beds = tabTd[0].string # first process neighborhood\n",
    "    # for all amounts \n",
    "    tabc = [None] * 3\n",
    "    for i, value in enumerate(tabTd[1:]):\n",
    "         tabc[i] = convertCurrency(value.string)\n",
    "\n",
    "#   #print (\"n = \", tabc)\n",
    "    bot25 = tabc[0]\n",
    "    median = tabc[1]\n",
    "    top25 = tabc[2]\n",
    "    \n",
    "    # insert the built postal code into the dataframe Neighborhood \tStudio \t \t2BR \tBudget\n",
    "    medianRent_df = medianRent_df.append({medianColNames[0] : beds,\n",
    "                                          medianColNames[1] : bot25,\n",
    "                                          medianColNames[2] : median,\n",
    "                                          medianColNames[3] : top25},\n",
    "                                         ignore_index=True)\n",
    "\n",
    "medianRent_df\n",
    "#df[df.columns[1:]].replace('[\\$,]', '', regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data in files for future usage\n",
    "filename = \"NYC_Average_Rents.csv\"\n",
    "if not_present(filename):\n",
    "    renthop_df.to_csv(filename)\n",
    "    print (filename, \" : created !\")\n",
    "\n",
    "filename = \"NYC_Median_Rents.csv\"\n",
    "if not_present(filename):\n",
    "    medianRent_df.to_csv(filename)\n",
    "    print (filename, \" : created !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good for the retrieved Average & Median Rents for registered neighborhoods.\n",
    "\n",
    "But we have found that NYC has 5 boroughs and 306 neighborhoods and here we have retrieved data for only 68 neighborhoods.\n",
    "\n",
    "We have to complete the missing values for all neighborhoods.\n",
    "\n",
    "I decided to get the \"Median Rents\" values from the same data source to complete the missing values !\n",
    "\n",
    "Let's go there..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below it can be used once the files have been already generated in the steps above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load walk scores from local file system in case we already created it in a previous step\n",
    "fromFile = False\n",
    "try:\n",
    "    renthop_df = pd.read_csv('NYC_Average_Rents.csv')\n",
    "    renthop_df.drop(renthop_df.columns[[0]], axis=1, inplace=True)\n",
    "    renthop_df.Budget = set_col_cat(renthop_df.Budget)\n",
    "    medianRent_df = pd.read_csv('NYC_Median_Rents.csv')\n",
    "    medianRent_df.drop(medianRent_df.columns[[0]], axis=1, inplace=True)\n",
    "    print('Loading NYC Average and Median Rents Data files Successful ! Skip BeautifulSoup !')\n",
    "    fromFile = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# If not already loaded redo all work at steps above (using Walk Score API to get the data)\n",
    "if not fromFile:\n",
    "    print ('Cannot load NYC Average and Median Rents Data files ! Use BeautifulSoup !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighborhood \tStudio \t1BR \t2BR \tBudget\n",
    "list_empty_avgrent = renthop_df.index[renthop_df['Studio'] == -1].tolist()\n",
    "print('Number of rows with empty rent value (=-1) = ', len(list_empty_avgrent), \n",
    "      '\\nlist = ', list_empty_avgrent, '\\n********\\n') #\n",
    "for index_avgrent in list_empty_avgrent:\n",
    "    studio = renthop_df.loc[index_avgrent]['Studio']\n",
    "    one_BR = renthop_df.loc[index_avgrent]['1BR']\n",
    "    name = renthop_df.loc[index_avgrent]['Neighborhood']\n",
    "    budget = renthop_df.loc[index_avgrent]['Budget']\n",
    "    print (\"Index = {0:5d} -> Studio = {1:5d} ,1BR = {2:5d}, Budget = {3:8s} -> Name = {4}\".\n",
    "           format(index_avgrent, int(studio), int(one_BR), budget, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have found 29 on 68 neighborhoods that have no price for 'Studio' category.\n",
    "\n",
    "We have to complete the missing values for all neighborhoods using the 'Budget' categories (['Average' 'Cheap' 'Pricey']).\n",
    "\n",
    "I decided to get the \"Median Rents\" values from the same data source to complete the missing values according to budget category. If the category is :\n",
    "- 'Cheap' then use 'Bot25%' value\n",
    "- 'Pricey' then use 'Top25%' value\n",
    "- 'Average' then use 'Median' value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighborhood \tStudio \t1BR \t2BR \tBudget\n",
    "def setMissingAvg(colName, rowIdx):\n",
    "    list_empty_avgrent = renthop_df.index[renthop_df[colName] == -1].tolist()\n",
    "    print('Number of rows with empty rent value (=-1) in colname {} = {}'.format(colName, len(list_empty_avgrent)), \n",
    "      '\\nlist = ', list_empty_avgrent, '\\n********\\n') #\n",
    "    \n",
    "    rent_bot = medianRent_df.loc[rowIdx]['Bot 25%']\n",
    "    rent_med = medianRent_df.loc[rowIdx]['Median']\n",
    "    rent_top = medianRent_df.loc[rowIdx]['Top 25%']\n",
    "\n",
    "    for index_avgrent in list_empty_avgrent:\n",
    "        budget = renthop_df.loc[index_avgrent]['Budget']\n",
    "        if (budget == 'Cheap'):\n",
    "            renthop_df.ix[index_avgrent, colName] = rent_bot\n",
    "        elif (budget == 'Average'):\n",
    "            renthop_df.ix[index_avgrent, colName] = rent_med\n",
    "        else:\n",
    "            renthop_df.ix[index_avgrent, colName] = rent_top\n",
    "\n",
    "setMissingAvg('Studio', 0)\n",
    "setMissingAvg('2BR', 2)\n",
    "\n",
    "renthop_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the generated values. it seems that they are not correctly set. \n",
    "\n",
    "We should think to a better approximation method.\n",
    "\n",
    "As example, a simple approximation algorithm could be : \n",
    "\n",
    "- For each row where value of Average[Studio] == -1 \n",
    "- Base on Budget \n",
    "     -> find Median[1BR] \n",
    "     -> define Ratio = (Average[row][1BR] / Median[1BR])\n",
    "     -> define Average[row][Studio] = Median[Studio] * Ratio\n",
    "\n",
    "Let's try again.\n",
    "We have to complete the missing values for all neighborhoods using the 'Budget' categories (['Average' 'Cheap' 'Pricey']).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighborhood \tStudio \t1BR \t2BR \tBudget\n",
    "def setMissingAvgRatio(colName, rowIdx):\n",
    "    list_empty_avgrent = renthop_df.index[renthop_df[colName] == -1].tolist()\n",
    "    print('Number of rows with empty rent value (=-1) in colname {} = {}'.format(colName, len(list_empty_avgrent)), \n",
    "      '\\nlist = ', list_empty_avgrent, '\\n********\\n') #\n",
    "    \n",
    "    rent_bot = medianRent_df.loc[rowIdx]['Bot 25%'] # 'Cheap'\n",
    "    rent_med = medianRent_df.loc[rowIdx]['Median']  # 'Average'\n",
    "    rent_top = medianRent_df.loc[rowIdx]['Top 25%'] # 'Pricey'\n",
    "\n",
    "    for index_avgrent in list_empty_avgrent:\n",
    "        # compute a ratio using '1BR' references\n",
    "        budget = renthop_df.loc[index_avgrent]['Budget']\n",
    "        if (budget == 'Cheap'):\n",
    "            val = (renthop_df.ix[index_avgrent, '1BR'] /\n",
    "                   medianRent_df.loc[1]['Bot 25%']) * rent_bot\n",
    "        elif (budget == 'Average'):\n",
    "            val = (renthop_df.ix[index_avgrent, '1BR'] / \n",
    "                   medianRent_df.loc[1]['Median']) * rent_med\n",
    "        else:\n",
    "            val = (renthop_df.ix[index_avgrent, '1BR'] /\n",
    "                   medianRent_df.loc[1]['Top 25%']) * rent_top\n",
    "        \n",
    "        # Apply the ratio to the colName value\n",
    "        val = round(val)\n",
    "        #print ('Current = {} -> corrected = {}'.\n",
    "        #       format(renthop_df.ix[index_avgrent, colName],\n",
    "        #              val))\n",
    "        renthop_df.ix[index_avgrent, colName] = val\n",
    "    \n",
    "# Try to load rental data from local file system in case we already created it in a previous step\n",
    "fromFile = False\n",
    "try:\n",
    "    renthop_df = pd.read_csv('NYC_Avg_Rents_Filled.csv')\n",
    "    renthop_df.drop(renthop_df.columns[[0]], axis=1, inplace=True)\n",
    "    renthop_df.Budget = set_col_cat(renthop_df.Budget)\n",
    "    fromFile = True\n",
    "    print('Loading NYC Average filled Data file Successful !')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# If not already loaded redo all work at steps above (using Walk Score API to get the data)\n",
    "if not fromFile:\n",
    "    print ('Cannot load NYC Average Filled and Median Rents Data files ! Using function !')\n",
    "    try:\n",
    "        renthop_df = pd.read_csv('NYC_Average_Rents.csv')\n",
    "        renthop_df.drop(renthop_df.columns[[0]], axis=1, inplace=True)\n",
    "        renthop_df.Budget = set_col_cat(renthop_df.Budget)\n",
    "        \n",
    "        medianRent_df = pd.read_csv('NYC_Median_Rents.csv')\n",
    "        medianRent_df.drop(medianRent_df.columns[[0]], axis=1, inplace=True)\n",
    "        print('Loading NYC Average filled and Median Rents Data files Successful !')\n",
    "        setMissingAvgRatio('Studio', 0)\n",
    "        setMissingAvgRatio('2BR', 2)\n",
    "        # Store these data\n",
    "        renthop_df.to_csv('NYC_Avg_Rents_Filled.csv')\n",
    "        print('Saving NYC Average filled file Successful !')\n",
    "    except:\n",
    "        print('Cannot load NYC Average and/or Median Rents Data file!\\n',\n",
    "              'Apply steps to rebuild data !')\n",
    "        pass\n",
    "\n",
    "\n",
    "renthop_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better (although we can continue to discute a more accurate correction method of course !). But for the moment we can go forward and add this data to the venues and see how we can achieve our goal.\n",
    "\n",
    "The problem we face now is also missing data for all NYC neighborhoods.\n",
    "We have only 68 neighborhoods out of 306 filled with rental data.\n",
    "Try to match the neighborhoods of the 2 sets.\n",
    "It seems we cannot match 34 over 68 neighborhoods from rental data to the NYC neighborhoods in the venues dataframe !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Total neighborhoods in NYC = {} , neighborhoods with rental data = {}'.\n",
    "       format(neighborhoods.shape[0], renthop_df.shape[0]))\n",
    "df_ws = df_venues_cleaned_wscores.drop(['Bike_score', 'Bike_descr', 'Transit_score','Transit_descr'], axis=1)\n",
    "df_rent = renthop_df.drop(['Studio','2BR'], axis=1)\n",
    "ws_nghrd_list = list(df_ws['Neighborhood'].unique())\n",
    "rent_nghrd_list  = list(renthop_df['Neighborhood'])\n",
    "\n",
    "df_rent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use geopy library to get the latitude and longitude values of rental neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our user_agent <em>ny_explorer</em> is reused as shown below to get localisation coordinates for the neighborhoods contained in rental data.\n",
    "Once localisation has been completed, clean and handle missing data if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_NYC = ', New York City, NY'\n",
    "geo_agent = \"ny_explorer\"\n",
    "geolocator = Nominatim(user_agent=geo_agent)\n",
    "\n",
    "latitude = []\n",
    "longitude = []\n",
    "unknown = 0\n",
    "# 'Neighborhood Latitude'  'Neighborhood Longitude'\n",
    "for address in rent_nghrd_list :\n",
    "    #time.sleep(0.300)\n",
    "    #print (\"Address is : \", address)\n",
    "    try:\n",
    "        location = geolocator.geocode(address + address_NYC)\n",
    "        lat = location.latitude\n",
    "        lng = location.longitude\n",
    "    except:\n",
    "        lat = -1\n",
    "        lng = -1\n",
    "        #print('  -> Cannot load NYC coordinate address!')\n",
    "        unknown += 1\n",
    "        pass\n",
    "    \n",
    "    latitude.append(lat)\n",
    "    longitude.append(lng)\n",
    "#    print('   -> The geograpical coordinate of {} are : {}, {}.'.format(address, lat, lng))\n",
    "print ('Unknown = ', unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = {'Neighborhood Latitude':  latitude, 'Neighborhood Longitude': longitude}\n",
    "df_rent = df_rent.assign(**location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_unknown_index = df_rent.index[df_rent['Neighborhood Latitude'] == -1].tolist()\n",
    "print (\"List of unknown Neighborhoods idexes : \", list_unknown_index)\n",
    "\n",
    "df_unknown_names = df_rent.iloc[list_unknown_index, 0].reset_index()\n",
    "list_unknown_names = df_unknown_names.iloc[:,1].tolist()\n",
    "print (\"List of unknown Neighborhoods names  : \", list_unknown_names)\n",
    "\n",
    "# [27, 42, 43, 50, 51, 55]\n",
    "#['Hunters Point', 'Northwestern Brooklyn', 'Northwestern Queens', \n",
    "#'Southwestern Queens', 'Stuyvesant Town - Peter Cooper Village', 'Theater District']\n",
    "df_rent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our user_agent <em>ny_explorer</em> is reused as shown below to get localisation coordinates for the neighborhoods contained in rental data.\n",
    "After localisation, we found 7 references in rental neighborhoods without localisation coordinates.\n",
    "\n",
    "Analyzing the results of rental data, it seems that some names represent sectors of NYC and covers several neighborhoods.\n",
    "\n",
    "Let's find this missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address_NYC = ', New York City, NY'\n",
    "geo_agent = \"ny_explorer\"\n",
    "geolocator = Nominatim(user_agent=geo_agent)\n",
    "\n",
    "def get_coords(ngh_list, df):\n",
    "    latitude = []\n",
    "    longitude = []\n",
    "    unknown = 0\n",
    "    # 'Neighborhood Latitude'  'Neighborhood Longitude'\n",
    "    n_name = ', ' + ngh_list[0]\n",
    "    #print (n_name, '  -> ',  ngh_list[1])\n",
    "    \n",
    "    for address in ngh_list[1] :\n",
    "        #time.sleep(0.300)\n",
    "        #print (\"Address is : \", address)\n",
    "        try:\n",
    "            location = geolocator.geocode(address + n_name + address_NYC)\n",
    "            lat = location.latitude\n",
    "            lng = location.longitude\n",
    "        except:\n",
    "            lat = -1\n",
    "            lng = -1\n",
    "            #print('  -> Cannot load NYC coordinate address!')\n",
    "            unknown += 1\n",
    "            pass\n",
    "    \n",
    "        latitude.append(lat)\n",
    "        longitude.append(lng)\n",
    "        #print('   -> The geograpical coordinate of {} are : {}, {}.'.format(address, lat, lng))\n",
    "    \n",
    "    print ('Unknown = ', unknown)\n",
    "    BR1_avg = df.ix[ngh_list[2],1]\n",
    "    budget = df.ix[2,2]\n",
    "    list_coords = []\n",
    "    \n",
    "    for i, name in enumerate(ngh_list[1]):\n",
    "        print(i, name, BR1_avg, budget, latitude[i], longitude[i])\n",
    "        list_coords.append([name, BR1_avg, budget, latitude[i], longitude[i]])\n",
    "\n",
    "    df_ngh = pd.DataFrame([item for item in list_coords])\n",
    "    df_ngh.columns = df.columns\n",
    "    df = df.append(df_ngh, ignore_index=True)\n",
    "    \n",
    "    return unknown, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the missing data, I had to :\n",
    " - use Web https://www.latlong.net/convert-address-to-lat-long.html for manually retrieve coordinates for 4 of these neighborhoods.\n",
    " - translate sectors of neighborhoods by the corresponding list of official neighborhoods and again retrieve coordinates for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neighborhood \t1BR \tBudget \tNeighborhood Latitude \tNeighborhood Longitude\n",
    "# Missing\n",
    "#[       27,                           42,                         43, \n",
    "#       50,                                51,                         55]\n",
    "#['Hunters Point' : done, 'Northwestern Brooklyn', 'Northwestern Queens', \n",
    "#'Southwestern Queens', 'Stuyvesant Town - Peter Cooper Village', 'Theater District']\n",
    "\n",
    "# These 4 coordinates have been manually retrieved via the \n",
    "# Web https://www.latlong.net/convert-address-to-lat-long.html\n",
    "Hunters_Point = [40.718804, -73.805154]       # 27 -> 'Queens',\n",
    "df_rent.ix[27, 3] = Hunters_Point[0]\n",
    "df_rent.ix[27, 4] = Hunters_Point[1]\n",
    "\n",
    "Peter_Cooper_Village =[40.734043, -73.977623] # 51 Northern Brooklyn Bedford-Stuyvesant \n",
    "df_rent.ix[51, 3] = Peter_Cooper_Village[0]\n",
    "df_rent.ix[51, 4] = Peter_Cooper_Village[1]\n",
    "\n",
    "Theater_District = [40.760213, -73.980105]    # 55 Midtown Manhattan\n",
    "df_rent.ix[55, 3] = Theater_District[0]\n",
    "df_rent.ix[55, 4] = Theater_District[1]\n",
    "\n",
    "# Try to retrieve these ones below via geocoder\n",
    "Northwestern_Brooklyn = ['Brooklyn',\n",
    "                       ['Brooklyn Heights','Brooklyn Navy Yard','Clinton Hill',\n",
    "                        'DUMBO', 'Downtown Brooklyn','Fort Greene','Prospect Heights',\n",
    "                        'Vinegar Hill'\n",
    "                       ],\n",
    "                      42]\n",
    "\n",
    "Northwestern_Queens = ['Queens',\n",
    "                       ['Corona','East Elmhurst','Glendale','Jackson Heights',\n",
    "                        'Maspeth','Middle Village','Ridgewood'],\n",
    "                      43]\n",
    "\n",
    "Southwestern_Queens = ['Queens',\n",
    "                       ['Forest Park', 'Howard Beach', 'Ozone Park', 'Richmond Hill',\n",
    "                        'South Ozone Park', 'Woodhaven'],\n",
    "                      50]\n",
    "\n",
    "print ('before size rent = ', df_rent.shape)\n",
    "df_rent_plus = df_rent.copy()\n",
    "\n",
    "u, df_rent_plus = get_coords(Northwestern_Brooklyn, df_rent_plus)\n",
    "u1, df_rent_plus = get_coords(Northwestern_Queens, df_rent_plus)\n",
    "u2, df_rent_plus = get_coords(Southwestern_Queens, df_rent_plus)\n",
    "\n",
    "print ('Unknown = ', u+u1+u2, ' -> after size rent = ', df_rent_plus.shape)\n",
    "\n",
    "df_rent_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete unknown location (get what we can from internet 'https://latitude.to')\n",
    "# - from renthp : get list of associated neighborhoods\n",
    "# - with geocoder retrieve associated locations\n",
    "# - then format and add these items to renthop dataframe\n",
    "# Last Remove Unknown #[27, 39, 42, 43, 50, 51, 55, 65]  [27, 42, 43, 50, 51, 55, 59] \n",
    "\n",
    "# Clean dataframe\n",
    "list_u_index = df_rent_plus.index[df_rent_plus['Neighborhood Latitude'] == -1].tolist()\n",
    "df_rent_plus = df_rent_plus.drop(list_u_index)\n",
    "df_rent_plus.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print ('Unknown = ', list_u_index, ' -> after cleaning rent_plus size = ', df_rent_plus.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_u_index = df_rent_plus.index[df_rent_plus['Neighborhood Latitude'] == -1].tolist()\n",
    "list_u_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now missing data has been completed, let's clean the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of neighborhood name duplicates =', \n",
    "      len(df_rent_plus['Neighborhood']) - len(df_rent_plus['Neighborhood'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we will need to remove the duplicated rows be sure that each neighborhood is uniquely defined !\n",
    "\n",
    "There are 4 neighborhoods names duplicated as seen above. Let's drop duplicated rows and reset index. See below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rent_plus.drop_duplicates(['Neighborhood'], inplace=True)\n",
    "df_rent_plus.reset_index(drop=True, inplace=True)\n",
    "print ('Number of neighborhood name duplicated after cleaning =', \n",
    "      len(df_rent_plus['Neighborhood']) - len(df_rent_plus['Neighborhood'].unique()),\n",
    "      ' new size = ', df_rent_plus.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rent_plus.Budget = df_rent_plus['1BR'].apply(lambda x: rescaleBudgetLabel(x))\n",
    "df_rent_plus.Budget = set_col_cat(df_rent_plus.Budget)\n",
    "\n",
    "filename = 'NYC_Renthop_Neighborhoods_with_coords_cleaned.csv'\n",
    "if not_present(filename):\n",
    "    df_rent_plus.to_csv('NYC_Renthop_Neighborhoods_with_coords_cleaned.csv')\n",
    "    print ('Created Clean Data file : NYC_Renthop_Neighborhoods_with_coords_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now we have a clean data set of neighborhoods with average rentals. \n",
    "\n",
    "To finish the data processing part, we need to merge the last dataframe containing the venues with walk scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rent_final =  pd.read_csv('NYC_Renthop_Neighborhoods_with_coords_cleaned.csv')\n",
    "df_rent_final.drop(df_rent_final.columns[[0]], axis=1, inplace=True)\n",
    "df_rent_final.Budget = df_rent_final['1BR'].apply(lambda x: rescaleBudgetLabel(x))\n",
    "df_rent_final.Budget = set_col_cat(df_rent_final.Budget)\n",
    "print(df_rent_final.dtypes, df_rent_final.shape)\n",
    "df_rent_final.sort_values(['1BR','Budget'],ascending=[False,False])\n",
    "# df_rent_final.to_csv('NYC_Renthop_Neighborhoods_with_coords_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_neighborhood = pd.merge(df_venues_cleaned_wscores, df_rent_final, on=['Neighborhood'], how='inner')\n",
    "merged_neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_neighborhood['Neighborhood'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 47 neighborhoods have matched ! \n",
    "\n",
    "**Because the names  are not exactly identical, venues for 38 neighborhoods have been lost so that we can only work on 2 on 11279 !**\n",
    "\n",
    "**I decided to process again the data but this time starting from the renthop file to retrieve the venues et perform the same work as previously done to obtain clean data ready for analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "target_root_categories = ['4d4b7105d754a06374d81259','4d4b7104d754a06370d81259',\n",
    "                          '4d4b7105d754a06373d81259','4d4b7105d754a06376d81259',\n",
    "                          '4d4b7105d754a06377d81259','4d4b7105d754a06378d81259'\n",
    "                         ]\n",
    "\n",
    "# define function to format venues address\n",
    "# Lot of problems with missing fields ex: 'address' or 'city'\n",
    "# -> so take formattedAddress\n",
    "def get_address(flocation):\n",
    "    # print ('location(address) = ', location['formattedAddress']) #['address'], location['city'])\n",
    "    address = flocation[0] + ', ' + flocation[1]\n",
    "    return address\n",
    "\n",
    "# build the list of chosen categories (easier to find a job)\n",
    "root_categories = target_root_categories[0]\n",
    "for cat in  target_root_categories[1:]:\n",
    "    root_categories = root_categories + ',' + cat\n",
    "\n",
    "# build the dataframe of selected categories of venues\n",
    "# Normally, we are limited by the API to 50 retrieved venues\n",
    "def getNearbyFilteredVenues(names, latitudes, longitudes,\n",
    "                            br1s, budgets,\n",
    "                            categories=\"\", radius=500, limit=100):\n",
    "    # Use search API with the list of selected root categories\n",
    "    url_str = \"https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&v={}&intent=browse&categoryId={}&ll={},{}&radius={}&limit={}\"\n",
    "\n",
    "    # Retrieve list of target venues in a radius of 500 m from the coordinates\n",
    "    # of each of the 306 identified neighborhoods and complying the given categories\n",
    "    venues_list=[]\n",
    "    i = 0\n",
    "    for name, lat, lng, br1, budget in zip(names, latitudes, longitudes, br1s, budgets):\n",
    "        #print(i, \" -> \", name)\n",
    "        time.sleep(0.1)\n",
    "        url = url_str.format(CLIENT_ID, CLIENT_SECRET, VERSION,\n",
    "                             categories,lat, lng, \n",
    "                             radius, 50)\n",
    "        try:\n",
    "            results = requests.get(url).json()[\"response\"][\"venues\"]\n",
    "        except:\n",
    "            print(i, \" -> \", name, \" -> No venues !!!!\")\n",
    "            continue\n",
    "            pass\n",
    "\n",
    "        print(i, \" -> \", name, \" -> venues number = \", len(results))\n",
    "\n",
    "        # For each resulting venue list for this neighborhood\n",
    "        # build the associated data : name address coordinates etc. \n",
    "        for v in results:\n",
    "            #parsed_venue = json.loads(venue)\n",
    "            #print(\"->\",json.dumps(v, indent=4, sort_keys=True))\n",
    "            #\n",
    "            venues_list.append([(\n",
    "                name,                       # neighborhood name\n",
    "                lat,                        # neighborhood latitude\n",
    "                lng,                        # neighborhood longitude\n",
    "                br1,                        # Avg 1BR price in neighborhood\n",
    "                budget,                     # Avg price ranking\n",
    "                v['name'],                  # venue name\n",
    "                get_address(v['location']['formattedAddress']), # venue address\n",
    "                v['location']['lat'],       # venue latitude\n",
    "                v['location']['lng'],       # venue longitude\n",
    "                v['categories'][0]['name']  # venue category\n",
    "            )])\n",
    "    \n",
    "        i = i +1\n",
    "\n",
    "    # Once we have registered all retrieved venues, put them in a new filtered dataframe\n",
    "    nearby_filtered_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "    nearby_filtered_venues.columns = ['Neighborhood',\n",
    "                                      'Neighborhood Latitude',\n",
    "                                      'Neighborhood Longitude',\n",
    "                                      '1BR', 'Budget',\n",
    "                                      'Venue',\n",
    "                                      'Address',\n",
    "                                      'Venue Latitude',\n",
    "                                      'Venue Longitude',\n",
    "                                      'Venue Category']\n",
    "\n",
    "    return nearby_filtered_venues\n",
    "\n",
    "print(\"Filtered Venues Functions defined !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load from local file system in case we did this before\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "print ('cwd =', cwd)\n",
    "\n",
    "fromFile = False\n",
    "try:\n",
    "    newyork_renthop_filtered_venues = pd.read_csv('./NYC_Renthop_Neighborhoods_filtered_venues.csv')\n",
    "    newyork_renthop_filtered_venues.drop(newyork_renthop_filtered_venues.columns[[0]], axis=1, inplace=True)\n",
    "    newyork_renthop_filtered_venues.Budget = set_col_cat(newyork_renthop_filtered_venues.Budget)\n",
    "    print('NYC Renthop Neighborhoods Filtered Venues data loaded.')\n",
    "    fromFile = True\n",
    "except:\n",
    "    pass\n",
    "\n",
    "#Neighborhood \t1BR \tBudget \tNeighborhood Latitude \tNeighborhood Longitude\n",
    "# If not already loaded use the Foursquare API to get the data \n",
    "if not fromFile:\n",
    "    print ('Data file for filtered venues not existing ! Call API')\n",
    "    newyork_renthop_filtered_venues = getNearbyFilteredVenues(names=df_rent_final['Neighborhood'],\n",
    "                                                              latitudes=df_rent_final['Neighborhood Latitude'],\n",
    "                                                              longitudes=df_rent_final['Neighborhood Longitude'],\n",
    "                                                              br1s=df_rent_final['1BR'],\n",
    "                                                              budgets=df_rent_final['Budget'],\n",
    "                                                              categories=root_categories,\n",
    "                                                              radius=500,\n",
    "                                                              limit=100\n",
    "                                                              )\n",
    "    # Export the dataframe values to a local file  \n",
    "    newyork_renthop_filtered_venues.to_csv('NYC_Renthop_Neighborhoods_filtered_venues.csv')\n",
    "\n",
    "newyork_renthop_filtered_venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of retrieved filtered venues is : ', newyork_renthop_filtered_venues.shape[0])\n",
    "newyork_renthop_filtered_venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve Walk Scores\n",
    "\n",
    "Following the format given above, thanks to the function below, I built the url from the parameters identifying a venue.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(address, lat, lon, wsapikey=\"\"):\n",
    "    url = 'http://api.walkscore.com/score?format=json&address={}&lat={}&lon={}&transit=1&bike=1&wsapikey={}'.format(\n",
    "            address, \n",
    "            lat,\n",
    "            lon,\n",
    "            wsapikey)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk Score Retrieval Functions Definition\n",
    "\n",
    "The functions below retrieve the walk scores and build the new dataframe with the added scores for each venue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the score of venues.\n",
    "# Normally, we are limited by the API to 5000 calls per day\n",
    "# An example of the results from WS API that we have to process\n",
    "result_str = '{\\\n",
    "    \"bike\": {\\\n",
    "        \"description\": \"Bikeable\",\\\n",
    "        \"score\": 54\\\n",
    "    },\\\n",
    "    \"description\": \"Somewhat Walkable\",\\\n",
    "    \"status\": 1,\\\n",
    "    \"walkscore\": 59\\\n",
    "}'\n",
    "\n",
    "\n",
    "def getScore(v_name, lat, lon, adr):\n",
    "    url = get_url(adr, lat, lon, wsapikey=WSAPIKEY)\n",
    "    scores_descs = [-1, \"\", -1, \"\", -1, \"\"]\n",
    "    results = requests.get(url).json() # json.loads(result_str) #  json.loads(result_str) # \n",
    "    status = results['status']\n",
    "    \n",
    "    bscore = tscore = -1\n",
    "    bdescr = tdescr = \"\"\n",
    "    if (status != 1):\n",
    "        print(\"Venue :\", v_name, \" -> Error status returned = \", status)\n",
    "    else :\n",
    "        #print(\"->\",json.dumps(results, indent=4, sort_keys=True))\n",
    "        wscore, wdescr = results['walkscore'],results['description']\n",
    "        try:\n",
    "            bscore, bdescr = results['bike']['score'], results['bike']['description']\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            tscore, tdescr = results['transit']['score'], results['transit']['description']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        scores_descs = [wscore, wdescr, bscore, bdescr, tscore, tdescr]\n",
    "\n",
    "    return scores_descs\n",
    "\n",
    "# Neighborhood Latitude \tNeighborhood Longitude \tVenue \tVenue Latitude \tVenue Longitude \tVenue Category\n",
    "def getWalkScoreVenue(venues_score_df,beg=0):\n",
    "    # Retrieve list of target venues in a radius of 500 m from the coordinates\n",
    "    # of each of the identified neighborhoods and compute walk score\n",
    "    \n",
    "    venues_df_ncol = len(venues_score_df.columns)\n",
    "    for index, row in venues_score_df.iterrows():\n",
    "        name = row['Venue']\n",
    "        lat = row['Venue Latitude']\n",
    "        lng = row['Venue Longitude']\n",
    "        adr = row['Address']\n",
    "        \n",
    "        #time.sleep(0.001)\n",
    "        results = getScore(name, lat, lng, adr)\n",
    "       \n",
    "        i = 0\n",
    "        #r_num = index - beg\n",
    "        venues_score_df.loc[index,'Walk_score'] = results[i] # wscore\n",
    "        i += 1\n",
    "        venues_score_df.loc[index,'Walk_descr'] = results[i] # wsdescr\n",
    "        #print('index = ', index, venues_score_df.loc[[index]])\n",
    "        if (index % 100) == 0:\n",
    "            print('index = ', index, venues_score_df.loc[index,'Walk_descr'], venues_score_df.loc[index,'Walk_score'])\n",
    "    \n",
    "    return venues_score_df\n",
    "\n",
    "# Try to load from local file system in case we did this before\n",
    "def load_scores(df_venues, beg, end):\n",
    "    filename = 'NYC_venues_walkability_{}_{}.csv'.format(beg,end)\n",
    "    fromFile = False\n",
    "    try:\n",
    "        df_venues = pd.read_csv(filename)\n",
    "        df_venues.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "        df_venues.Budget = set_col_cat(df_venues.Budget)\n",
    "        df_venues.Walk_descr = set_col_cat(df_venues.Walk_descr, ws_cat)\n",
    "        print('NYC Neighborhoods Venues Walkability Scores data loaded from file :', \n",
    "              filename)\n",
    "        \n",
    "        fromFile = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # If not already loaded : use the Foursquare API to get the data\n",
    "    if not fromFile:\n",
    "        print (filename, ' : Data file for filtered venues not existing ! Call API')\n",
    "        # Add scores columns for walkability and bike\n",
    "        df_venues = df_venues.assign(Walk_score=-1, Walk_descr=None)\n",
    "        df_venues = getWalkScoreVenue(df_venues,beg)\n",
    "        df_venues.Budget = set_col_cat(df_venues.Budget)\n",
    "        df_venues.Walk_descr = set_col_cat(df_venues.Walk_descr, ws_cat)\n",
    "\n",
    "        # Export the dataframe values to a local file  \n",
    "        #print(df)\n",
    "        #df_venues.to_csv(filename)\n",
    "        print ('Venues with Walk Scores Data file created : ', filename)\n",
    "    return df_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WSAPIKEY = cred_df.iat[3, 0]\n",
    "#df_0_3 = pd.DataFrame()\n",
    "df_0_3 = newyork_renthop_filtered_venues[0:3].copy()\n",
    "df_0_3 = load_scores(df_0_3, 0, 3)\n",
    "df_0_3.dtypes\n",
    "df_0_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reworking \n",
    "Looking at the budget with 3 values, sometimes a 1BR amount is labelled for example 'Pricey' while another higher amount is labelled 'Average'. It is not very consistent.\n",
    "\n",
    "It is why, I think it is more relevant to rescale this column by associating a more understandable label to the price '1BR' value.\n",
    "\n",
    "So we will rework the Budget column so that new categories will remap on this table :  \n",
    "\n",
    "|  Label    |    Inf    |  Sup  |\n",
    "|:---------:|:---------:|:-----:|\n",
    "| *Cheap*   |     0     | 2149  |\n",
    "| *Average* |    2150   | 3149  |\n",
    "| *Pricey*  |    3150   |  '>'  |\n",
    "\n",
    "So, let's compute and associate a categorical label to the registered Budget value.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WSAPIKEY = cred_df.iat[3, 0]\n",
    "df_0_3513 = pd.DataFrame()\n",
    "df_0_3513 = load_scores(newyork_renthop_filtered_venues[0:].copy(), 0, 3513)\n",
    "df_0_3513.drop(df_0_3513.columns[[0]], axis=1, inplace=True)\n",
    "df_0_3513['Walk_descr']= df_0_3513['Walk_score'].apply(lambda x: getWalkLabel(x))\n",
    "df_0_3513.Walk_descr = set_col_cat(df_0_3513.Walk_descr, ws_cat)\n",
    "\n",
    "df_0_3513.Budget = df_0_3513['1BR'].apply(lambda x: rescaleBudgetLabel(x))\n",
    "df_0_3513.Budget = set_col_cat(df_0_3513.Budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_0_3513.to_csv('NYC_venues_walkability_0_3513.csv')\n",
    "df_venues.sort_values(['1BR','Budget'],ascending=[False,False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataframe\n",
    "list_u_index = df_0_3513.index[df_0_3513['Walk_score'] == -1].tolist()\n",
    "print(list_u_index)\n",
    "if (len(list_u_index) > 0):\n",
    "    df_0_3513 = df_0_3513.drop(list_u_index)\n",
    "    df_0_3513.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Data Processing \n",
    "\n",
    "Normally at this step, all venues have been collected and processed : data cleaning, missing data etc...\n",
    "For convenience reasons, this data will be stored in a file that can be reloaded in a dataframe at any time for future operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'NYC_venues_walkability_{}_{}.csv'.format(0,3513) #NYC_venues_walkability_0_3513.csv\n",
    "df_venues = pd.read_csv(filename)\n",
    "df_venues.drop(df_venues.columns[[0]], axis=1, inplace=True)\n",
    "df_venues['Walk_descr']= df_venues['Walk_score'].apply(lambda x: getWalkLabel(x))\n",
    "df_venues['Budget']= df_venues['1BR'].apply(lambda x: rescaleBudgetLabel(x))\n",
    "df_venues.Budget = set_col_cat(df_venues.Budget)\n",
    "df_venues.Walk_descr = set_col_cat(df_venues.Walk_descr, ws_cat)\n",
    "\n",
    "print('NYC Neighborhoods Venues Walkability Scores data loaded from file :', filename)\n",
    "print(df_venues.dtypes, '\\n',' -> shape = ', df_venues.shape)\n",
    "df_venues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can summarize what we know about the new dataframe with the added scores for each venue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Data Summary\\n------------\\n')\n",
    "print('Total number of {:<26} = {} '.format('venues', df_venues.shape[0]))\n",
    "print('Total number of {:<26} = {} '.format('venue attributes', df_venues.shape[1]))\n",
    "print('Total number of {:<26} = {} '.format('represented neighborhoods', len(df_venues['Neighborhood'].unique())))\n",
    "print('Total number of {:<26} = {} '.format('unique venue categories', len(df_venues['Venue Category'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks good and that ends the data gathering and processing phases.\n",
    "\n",
    "Hard work and a lot of trials have been necessary to retrieve with APIs and sometimes manually the full information needed to our analysis.\n",
    "\n",
    "#### Conclusion : data retrieval and processing\n",
    "\n",
    "So now we have all the venues in the retrieved set of 3508 samples representing 81 neighborhoods in New York area with walk scores and rental data.\n",
    "\n",
    "In the present study, due to lack of time, we will only consider walk score, not bike or transit scores that are also useful information needed to refine the results we would like to present to the interested stakeholders.\n",
    "\n",
    "We also could extend the information to all 305 neighborhoods of NYC but it needed more work to retrieve for example exhaustive rental information.\n",
    "\n",
    "Idem for the number of venues limited for our profile to 50 per search call and walkability scores limited to 5000 per day that is clearly not sufficient in time to cover all potential venues existing in all neighborhoods of NYC.  \n",
    "\n",
    "The strategy concerning missing data, duplicates could also be refined : dropping is not always the more efficient action ! We tried also to calculate approximate values but from my point of view, the results are not entirely satisfactory.  Developing more elaborated algorithms needs more time I was not able to dedicate to this study.\n",
    "\n",
    "This concludes our data gathering phase. \n",
    "\n",
    "Let's continue to the next step : how this data will be use for analysis leading to find the best places in the 81 neighborhoods of NYC for a student or person wanting to rent a 1BR apartment in a neighborhood with potential work places easily accessible on foot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will direct our efforts on detecting areas of New York City that have the highest density of venues located in the highest walk-scored places with the lowerest rental rates.\n",
    "\n",
    "As said in the _**Data**_ section , we will limit our analysis to the ares containes in the 81 neighborhoods of NYC having information about walkability scores and rental rates.\n",
    "\n",
    "Below, I detail the methodology I used to carry out this project.\n",
    "\n",
    "- Step 1:\n",
    "\n",
    "    Previously, in a we have already collected the amount of **data: location, category, walk score and average rent for each potential workplace located in 81 neighborhoods of New York City. Each potential workplace have been identified and classified according to Foursquare categorization**.\n",
    "\n",
    "\n",
    "- Step 2:\n",
    "\n",
    "    The analysis will be based on calculation and exploration of '**venues**' across the different neighborhoods of NYC. We will use **heatmaps** to identify a few promising areas with highest density of venues. walkscores and the lowest rental values and focus our attention on those areas.\n",
    "\n",
    "\n",
    "- Step 3:\n",
    "\n",
    "    In this final step we will focus on most promising areas and within those create **clusters of locations that meet the requirements** established at the beginning of the project defined with stakeholders.\n",
    "    \n",
    "    The locations targeted are those with the highest '**walk scores**' and the '**lowest rates of rent**'. These areas will be classified according to the density of venues in the  vicinity having the highest rate of **pedestrian friendliness in a radius of 500 meters**,  and we want locations **with the lowest rates of rent in radius of 500 meters**.\n",
    "    \n",
    "    A map will be presented to the stakeholders, displaying all such locations grouped in createed clusters (using **k-means clustering**) of those locations to identify these specific target areas in neighborhoods which constitute a starting directory of adresses to be explored and searched by the astakeholders identified as the best places to choose the cheapest home and find work in the nearest vicinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis <a name=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's perform some basic explanatory data analysis and derive some additional info from our raw data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3508 retrieved venues that are completely filled by rental data !\n",
    "Let's see the number of venues grouped by neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_venues.groupby('Neighborhood', as_index=False).count() #\n",
    "df = df.sort_values('Venue',ascending=False).reset_index(drop=True)\n",
    "df['Venues CumSum'] = df['Venue'].cumsum()\n",
    "df[['Neighborhood','Venue', 'Venues CumSum']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know that the first 53 elements in range [0, 52] (-> Garment District) have all 50 retrieved venues and cumulate from 0 to 2650 venues on a total of 3508 for all 81 venues.\n",
    "\n",
    "Neighborhoods in last range [54, 80]  (Battery Park City -> Forest Park) cumulate from 2651 to 3508 venues\n",
    "\n",
    "Unfortunately we have no information on real repartition of venues because the most part of neighborhoods that cumulate more than 50 retrieved venues are limited to 50 cause API limitation.\n",
    "\n",
    "So let's try some visualization on the retrieved data.\n",
    "\n",
    "Let's now compute the walkability for these neighborhoods in this order.\n",
    "\n",
    "- loop on neighborhoods of NYC\n",
    "   - loop on each venue\n",
    "     - compute walkability \n",
    "     - add walk '(bike/transit) score values in new columns of newyork_filtered_venues\n",
    "   - compute mean values by neighborhood if needed\n",
    "  \n",
    "Discussion :\n",
    " - we can perform the call for each of the 11311 retrieved venues (need at least 3 days)\n",
    " - we can just perform the walkability calculation on the first 5000 venues but 143 neighborhoods have 50 venues\n",
    " - we can pick a number of venues according to the density of retrieved venues for each neighborhood so that\n",
    "   all neighborhoods are covered and can be compared in term of walkability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_venues.copy()\n",
    "df = df.groupby('Neighborhood', as_index=False).count().sort_values('Venue',ascending=True).reset_index(drop=True)#\n",
    "#df = df.sort_values('Venue',ascending=False).reset_index(drop=True)\n",
    "df['Venues CumSum'] = df['Venue'].cumsum()\n",
    "df['Number Venues'] = df['Venue']\n",
    "del df['Venue'] # , 'Address', 'Venue Latitude' , 'Venue Longitude' , 'Venue Category' , 'Walk_score' , 'Walk_descr']\n",
    "lst = ['Address', 'Venue Latitude' , 'Venue Longitude' , 'Venue Category' , 'Walk_score' , 'Walk_descr']\n",
    "for item in lst:\n",
    "    del df[item]\n",
    "df = df.sort_values('Neighborhood',ascending=True).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Categorical Information at Neighborhood level : Mean Walk Score\n",
    "\n",
    "###### How Walk Score Works\n",
    "\n",
    "Walk Score helps you find a walkable place to live. Walk Score is a number between 0 and 100 that measures the walkability of any address.\n",
    "\n",
    "The sWalk score in a number indicating a category of place as described below :\n",
    "\n",
    "- 90–100\t: **[Walker’s Paradise]**\n",
    "              Daily errands do not require a car\n",
    "- 70–89\t    : **[Very Walkable]**\n",
    "              Most errands can be accomplished on foot\n",
    "- 50–69\t    : **[Somewhat Walkable]**\n",
    "              Some errands can be accomplished on foot\n",
    "- 25–49\t    : **[Car-Dependent]**\n",
    "              Most errands require a car\n",
    "- 0–24\t    : **[Car-Dependent]**\n",
    "              Almost all errands require a car\n",
    "\n",
    "Let's compute and associate a categorical label to the registered neighborhoods relative to the walk score of each retrieved venues.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1= df_venues.copy()\n",
    "#df['WS_Mean']= df.groupby('Neighborhood')['Walk_score'].transform('mean')#\n",
    "df_ws_mean = df1.groupby('Neighborhood').agg('mean').reset_index()\n",
    "df_ws_mean = df_ws_mean.sort_values('Neighborhood',ascending=True)\n",
    "df_ws_mean = df_ws_mean[['Neighborhood', 'Walk_score']]\n",
    "df_ws_mean[['Number Venues']]= df[['Number Venues']]\n",
    "df_ws_mean.rename(index=str, columns={\"Walk_score\": 'WS_mean'}, inplace=True)\n",
    "df_ws_mean.WS_mean = df_ws_mean.WS_mean.round().astype(int)\n",
    "df_ws_mean['WS_descr']= df_ws_mean['WS_mean'].apply(lambda x: getWalkLabel(x))\n",
    "df_ws_mean.WS_descr = set_col_cat(df_ws_mean.WS_descr, ws_cat)\n",
    "df_ws_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.style.use('ggplot') # optional: for ggplot-like style\n",
    "\n",
    "# check for latest version of Matplotlib\n",
    "print ('Matplotlib version: ', mpl.__version__) # >= 2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's build or reload raw data needed for visualization and basic  explanatory data analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_info_neighborhood():\n",
    "    df_rent_final =  pd.read_csv('NYC_Renthop_Neighborhoods_with_coords_cleaned.csv')\n",
    "    df_rent_final.drop(df_rent_final.columns[[0]], axis=1, inplace=True)\n",
    "    df_rent_final.Budget = set_col_cat(df_rent_final.Budget)\n",
    "    #print(df_rent_final.dtypes)\n",
    "\n",
    "    filename = 'NYC_Information_Viz.csv'\n",
    "    if not not_present(filename):\n",
    "        df_info_viz = pd.read_csv('NYC_Information_Viz.csv')\n",
    "        df_info_viz.drop(df_info_viz.columns[[0]], axis=1, inplace=True)\n",
    "        df_info_viz.reset_index(drop=True)\n",
    "        df_info_viz.Budget = set_col_cat(df_info_viz.Budget, budget_cat)\n",
    "        df_info_viz.WS_descr = set_col_cat(df_info_viz.WS_descr, ws_cat)\n",
    "        print('Data ready for visualisation reloaded from : ', filename)\n",
    "    else :\n",
    "        print(filename, ' not existing ! Data for visualisation to be rebuild from previous steps !')\n",
    "        df_info_viz = df_rent_final.copy().sort_values('Neighborhood',ascending=True)\n",
    "        df_ws_mean, df_info_viz = [d.reset_index(drop=True) for d in (df_ws_mean, df_info_viz)]\n",
    "        df_ws_mean.index =  df_info_viz.index\n",
    "        df_info_viz = df_info_viz.join(df_ws_mean[['Number Venues', 'WS_mean', 'WS_descr']]).reset_index(drop = True)\n",
    "        df_info_viz.to_csv('NYC_Information_Viz.csv')\n",
    "\n",
    "    # or pd.concat([df_info_viz, df_ws_mean[['Number Venues', 'WS_Mean']]], axis=1)\n",
    "    df_info_viz.sort_values(['WS_descr', 'Budget', 'WS_mean'], ascending=[False,True,False], axis=0, inplace=True)\n",
    "    \n",
    "    return df_info_viz\n",
    "    \n",
    "df_info_viz = build_info_neighborhood()\n",
    "print(df_info_viz.dtypes, len(df_info_viz.index))\n",
    "df_info_viz.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Graphical Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_info_viz.sort_values(['Number Venues'], ascending=False, axis=0, inplace=True)\n",
    "count, bin_edges = np.histogram(range(df_info_viz.shape[0]), bins =8)\n",
    "print(count) # frequency count\n",
    "print(bin_edges) # bin ranges, default = 10 bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_viz['Index'] = range(df_info_viz.shape[0])\n",
    "df_info_viz.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  a. Venue Density "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_viz['Number Venues'].plot(kind='hist', figsize=(8, 6))\n",
    "plt.title('Histogram of number of venues retrieved in NYC Neighborhoods') # add a title to the histogram\n",
    "plt.ylabel('Number of Neighborhoods') # add y-label\n",
    "plt.xlabel('Number Venues') # add x-label\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df_info_viz.copy()\n",
    "df_t.set_index(['Index'], inplace=True)\n",
    "df_t = df_t.loc[df_t.index, 'Number Venues']\n",
    "df_t.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate histogram\n",
    "df_t.plot(kind='bar', figsize=(10, 6))\n",
    "\n",
    "plt.title('Histogram of number of Venues by neighborhood in NYC')\n",
    "plt.ylabel('Number of venues')\n",
    "plt.xlabel('Neighborhood Index')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t = df_info_viz.copy()\n",
    "#df_t.set_index(['Neighborhood'], inplace=True)\n",
    "df_t.sort_values(['1BR'], ascending=False, axis=0, inplace=True)\n",
    "df_t.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  b. Rental Prices in Neighborhoods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Bar\n",
    "count, bin_edges = np.histogram(df_t['1BR'])\n",
    "df_t['1BR'].plot(kind='hist', figsize=(8, 5))\n",
    "\n",
    "plt.title('Histogram of Average 1BR Rental by neighborhood in NYC')\n",
    "plt.ylabel('Number of neighborhoods')\n",
    "plt.xlabel('Average 1BR Rental')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate Bar\n",
    "count, bin_edges = np.histogram(df_t['1BR'])\n",
    "df_t['1BR'].plot(kind='bar', figsize=(8, 5))\n",
    "\n",
    "plt.title('Histogram of Average 1BR Rental by neighborhood in NYC')\n",
    "plt.ylabel('Average 1BR Rental')\n",
    "plt.xlabel('Neighborhood Index')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iv= df_info_viz[['Neighborhood','1BR','Budget']].copy()\n",
    "df_iv['1BR'] = df_iv['1BR'].astype(int)\n",
    "df_iv.sort_values(['1BR'], ascending=False, axis=0, inplace=True)\n",
    "df_iv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_br = df_iv.copy()\n",
    "df_br.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  c. Statistics Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go further in our explanatory data analysis and see if some additional information from our raw data is needed. \n",
    "\n",
    "First let's calculate some basics statistics on rentals over the neighborhoods. That could be useful information foor the stakeholders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1. Rental Prices Description  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe summary statistics on 1BR over the neighborhoods\n",
    "print (df_info_viz['1BR'].describe())\n",
    "\n",
    "(df_info_viz['1BR']).plot(kind='box', figsize=(8, 6))\n",
    "\n",
    "plt.title('Box plot of Average Rentals of 1 Bedroom apartment')\n",
    "plt.ylabel('1BR Avg price')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Minimum: Smallest number in the dataset                            : min  = 1625\n",
    "- First quartile: Middle number between the minimum and the median   : 25%  = 2250\n",
    "- Second quartile (Median): Middle number of the (sorted) dataset    : 50%  = 2950\n",
    "- Third quartile: Middle number between median and maximum           : 75%  = 3312\n",
    "- Maximum: Highest number in the dataset                             : max  = 4391\n",
    "\n",
    "It has been counted on 81 values and the mean Average Rental values is around $2798.\n",
    "\n",
    "That seems to be a quite high amount of money for a student ! \n",
    "\n",
    "We could also see how the neighborhood are considered relative to the 'Budget' value, a category indicating the observed rental price compared to what it should be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Rental Price Percentages Categories Description  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb = df_iv.copy()\n",
    "df_bbg  = df_bb.groupby('Budget', as_index=False).count().sort_values('Budget',ascending=True).reset_index(drop=True) #\n",
    "df_bbg = df_bbg[['Budget', 'Neighborhood']]\n",
    "total_n = df_bbg['Neighborhood'].sum()\n",
    "df_bbg['Perc%'] = ((df_bbg[['Neighborhood']] / total_n) * 100).round()\n",
    "print ('Percentage of Neighborhoods per Price category')\n",
    "df_bbg[['Budget','Perc%']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_bbg['Perc%'].describe())\n",
    "\n",
    "df_bbg['Perc%'].plot(kind='box', figsize=(8, 6))\n",
    "\n",
    "plt.title('Box plot of Neighborhoods Percentages by Budget category')\n",
    "plt.ylabel('Percentage of Neighborhoods')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3. Rental Price Repartition by  Categories Description  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb = df_iv.copy()\n",
    "df_bb['1BR'] = (df_info_viz['1BR']).astype(int)\n",
    "df_bb = df_bb.sort_values(['Budget', '1BR'] ,ascending=[True,True])\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.catplot(x=\"Budget\", y=\"1BR\", data=df_bb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore now data for Average rental by Walkability scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  d. Walkability and Rental Prices Relationship  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ws_cat = ['Car-Dependant', 'Somewhat Walkable', 'Very Walkable', 'Walker’s Paradise']\n",
    "#budget_cat =['Cheap', 'Average', 'Pricey']\n",
    "dd = df_info_viz[['Budget', 'WS_descr', 'Number Venues']].copy()\n",
    "df_11 = pd.DataFrame([], index=budget_cat, columns=ws_cat)\n",
    "df_11.reset_index(drop=True)\n",
    "df_11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1. Neighborhoods by Walkability Scores and Rental Price Categories  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd_ngh = df_11.copy()\n",
    "dd = df_info_viz[['Budget', 'WS_descr', 'Number Venues']].copy()\n",
    "dfd_ngh.apply(calc_total, args =('Count',dd), axis=1)\n",
    "df_nac =  dfd_ngh.loc[['Average','Cheap'],['Very Walkable','Walker’s Paradise']]\n",
    "total_n = dfd_ngh.astype(float).sum().sum().astype(int)\n",
    "print ('Total Neighborhoods = ', total_n)\n",
    "total_ac_vp = df_nac.astype(float).sum().astype(int)\n",
    "print('Total number of neighborhoods with lowest Rentals and more than Very Walkable = ', total_ac_vp.sum(),\n",
    "      '\\n -> Representing a rate of : ', str(round(total_ac_vp.sum()/total_n * 100).astype(int))+'%')\n",
    "print()\n",
    "dfd_ngh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd_ngh_perc = dfd_ngh.copy()\n",
    "df_nac_perc =  dfd_ngh_perc.loc[['Average','Cheap'],['Very Walkable','Walker’s Paradise']]\n",
    "total_n = dfd_ngh_perc.astype(float).sum().sum().astype(int)\n",
    "total_ac_vp_perc = df_nac_perc.astype(float).sum().sum().astype(int)\n",
    "print ('Total Neighborhoods = ', total_n)\n",
    "print ('Total affordable Neighborhoods in more than Very Walkable areas = ', total_ac_vp_perc)\n",
    "print('Total Percentage of affordable Neighborhoods in top 2 walkable areas = ', \n",
    "      str(round(total_ac_vp_perc.sum()/total_n * 100).astype(int)) +\n",
    "      '%')\n",
    "dfd_ngh_perc= dfd_ngh_perc.divide(total_n).mul(100).astype(float).round(0)\n",
    "print('\\n-------------------------------------------------------------------\\n')\n",
    "print('Table of Percentages of Neighborhood Walkability Scores by Areas Prices')\n",
    "dfd_ngh_perc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we can see clearly that more than half (45 on 81) i.e 56% of all neighborhoods prices are relatively affordable. \n",
    "\n",
    "All 'Pricey' neighborhoods (33 on 81) are highly walkable and represent about 41% of the total.\n",
    "\n",
    "Let's repeat the same calculus to explore now the number of venues for potential workplaces relative to Walkability scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Venues Targeted by Walkability Scores and Rental Price Categories  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd_nv = df_11.copy()\n",
    "\n",
    "dfd_nv.apply(calc_total, args=('Number Venues', df_info_viz[['Budget', 'WS_descr', 'Number Venues']].copy()), axis=1)\n",
    "df_ac =  dfd_nv.loc[['Average','Cheap'],['Very Walkable','Walker’s Paradise']]\n",
    "\n",
    "total_v= dfd_nv.loc[['Cheap','Average','Pricey'],['Very Walkable','Walker’s Paradise']].astype(float).sum().sum().astype(int)\n",
    "print ('Total Venues Very Walkable or Walker\\'s Paradise = ', total_v)\n",
    "total_ac_vp = df_ac.astype(float).sum().astype(int)\n",
    "print('Total number of venues with lowest Rentals and more than Very Walkable = ', total_ac_vp.sum(),\n",
    "      '\\n -> Representing a rate of : ', str(round(total_ac_vp.sum()/total_v * 100).astype(int))+'%')\n",
    "#dfd_nv\n",
    "dfd_nv.loc[['Average','Cheap'],['Very Walkable','Walker’s Paradise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def numpy_numexpr_app(df):\n",
    "#    a = df.values.astype(float)\n",
    "#    N =  total_v\n",
    "#    return np.around(pd.DataFrame(a/N, columns=df.columns, index=df.index),decimals=2)\n",
    "\n",
    "#dfd_nv_p = numpy_numexpr_app(dfd_nv)\n",
    "#dfd_nv_p.head(6)\n",
    "\n",
    "def format_x(x):\n",
    "    x =float(x)\n",
    "    if not math.isnan(x):\n",
    "        val = str(x).replace('.0', '%')\n",
    "    else:\n",
    "        val ='-'\n",
    "    return val\n",
    "\n",
    "dfd_nv_perc = dfd_nv.copy()\n",
    "\n",
    "dfd_vac_perc =  dfd_nv_perc.loc[['Average','Cheap'],['Very Walkable','Walker’s Paradise']]\n",
    "#total_v = dfd_nv_perc.astype(float).sum().sum().astype(int)\n",
    "\n",
    "total_vac_vp_perc = dfd_vac_perc.astype(float).sum().sum().astype(int)\n",
    "print ('Total Venues Very Walkable or Walker\\'s Paradise = ', total_v)\n",
    "print ('Total affordable Venues in more than Very Walkable areas = ', total_vac_vp_perc)\n",
    "print('Total Percentage of Venues in top 2 walkable areas = ', \n",
    "      str(round(total_vac_vp_perc.astype(int)/total_v * 100).astype(int)) +\n",
    "      '%')\n",
    "dfd_nv_perc= dfd_nv_perc.divide(total_v).mul(100).astype(float).round(0)\n",
    "print('\\n-------------------------------------------------------------------\\n')\n",
    "print('Table of Percentages of Walkability Scores by Areas Prices')\n",
    "dfd_nv_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As for the chart, use a figure size of (20, 8):\n",
    "# - bar width of 0.8,\n",
    "# - use color #5cb85c for the Very interested bars, \n",
    "#       color #5bc0de for the Somewhat interested bars, and \n",
    "#       color #d9534f for the Not interested bars,\n",
    "# - use font size 14 for the bar labels, percentages, and legend,\n",
    "# - use font size 16 for the title, and,\n",
    "# - display the percentages above the bars as shown above, and \n",
    "#   remove the left, top, and right borders.\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from decimal import Decimal\n",
    "\n",
    "mpl.style.use('ggplot') # optional: for ggplot-like style\n",
    "\n",
    "w_vi = 'chartreuse'         # 'Walker’s Paradise'\n",
    "v_ni = 'springgreen'        # 'Very Walkable'\n",
    "s_si = 'olive'              # 'Somewhat Walkable'\n",
    "c_ci = 'darkgreen'          # 'Car-Dependant'\n",
    "\n",
    "colors= list([c_ci, s_si, v_ni, w_vi])\n",
    "\n",
    "# create bar plot\n",
    "ax = dfd_nv_perc.plot(kind='bar', width=0.8, color=colors, figsize=(20, 8))\n",
    "\n",
    "# set title and legend\n",
    "# add title to the plot\n",
    "ax.set_title('Percentage of Walkability Scores by Areas Prices', fontsize=16, fontweight=\"bold\") \n",
    "leg = ax.legend(frameon=1, prop=dict(size=14,weight='bold'))\n",
    "leg.get_frame().set_linewidth(2.0)\n",
    "\n",
    "for label in ax.xaxis.get_ticklabels():\n",
    "    label.set_fontsize(16)\n",
    "    label.set_weight('bold')\n",
    "\n",
    "# Remove vertical axe\n",
    "plt.gca().axes.get_yaxis().set_visible(False)\n",
    "\n",
    "# Add bottom line\n",
    "plt.axhline(0, linewidth = 5.0, color='grey')\n",
    "\n",
    "for p in ax.patches:\n",
    "#    strperc = '{:^15}'.format(str(p.get_height()).split('.',1)[1]+ '%') with 0.68\n",
    "    strperc = '{:^15}'.format(str(int(p.get_height()))+ '%') # with 68\n",
    "    ax.annotate(strperc, (p.get_x(), p.get_height() + 0.6), fontsize=15, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all areas are massively Wakable and the highest is walkability score, the highest is the rental price.\n",
    "\n",
    "So, the calculated numbers for venues correlate quite well those for neighborhoods (not a surprise, the opposite would have been surprising of course !). About 53% of all venues are located in 'affordable' areas with high walkability scores.\n",
    "\n",
    "As previously observed, all venues located in 'Pricey' areas are also highly walkable and represent about 46% of the total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_viz.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bb.sort_values(['1BR', 'Budget'] ,ascending=[True,True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok let's see another plotting results !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ws_viz = df_info_viz.copy()\n",
    "df_ws_viz = df_ws_viz.sort_values(['Budget','1BR'] ,\n",
    "                                  ascending=[False,True]).reset_index(drop=True)\n",
    "df_ws_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.  Rental Price Categories  by Neighborhood Walkability Score Mean "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is plotting observed Pricing relative to Walkability mean in the Neighborhood using **catplot** !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ws_viz = df_info_viz.copy().sort_values('Neighborhood',ascending=True).reset_index(drop=True)\n",
    "\n",
    "df_ws_viz['1BR'] = df_ws_viz['1BR'].astype(int)\n",
    "df_ws_viz = df_ws_viz.sort_values(['Budget','1BR'] ,\n",
    "                                  ascending=[True,True]).reset_index(drop=True)\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.catplot(x=\"Budget\", y=\"WS_mean\", data=df_ws_viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below the same as above but using **boxplot** !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset(\"tips\")\n",
    "#ax = sns.catplot(x=\"WS_descr\", y=\"1BR\", data=df_ws_viz)\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.boxplot(x=\"Budget\", y=\"WS_mean\", data=df_ws_viz)\n",
    "#ax = sns.boxplot(x=\"WS_mean\", y=\"Budget\", data=df_ws_viz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4.  Neighborhood Walkability Score Mean  by '1BR' Rental Prices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a **boxplot** to vizualize Walk score by 1 Bedroom-homes prices !\n",
    "\n",
    "Well we can easily observe that the most highly wakable places are also the most costly ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips = sns.load_dataset(\"tips\")\n",
    "#ax = sns.catplot(x=\"WS_descr\", y=\"1BR\", data=df_ws_viz)\n",
    "plt.figure(figsize=(8,6))\n",
    "ax = sns.boxplot(x=\"WS_descr\", y=\"1BR\", data=df_ws_viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ws_viz.sort_values(['WS_mean','1BR', 'Neighborhood'], ascending=[False,True,True]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venues_sorted = df_venues.copy().sort_values(['Walk_score','1BR', 'Venue Latitude', 'Venue Longitude','Neighborhood'],\n",
    "                                                        ascending=[False,True,True,True,True]).reset_index(drop=True)\n",
    "df_venues_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  5. Conclusions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting information for the future tenants ! \n",
    "\n",
    "- Unfortunately major part of the rents are 'Pricey' : 41 %\n",
    "- 1BRs with a Cheap rent are not so many : 22 %\n",
    "- Even in Average prices, only a little bit more than 1/3 can be found : 41 %\n",
    "\n",
    "We already found that :\n",
    "    Minimun: Smallest number in the dataset : min = 1625\n",
    "    First quartile: Middle number between the minimum and the median : 25% = 2250\n",
    "    Second quartile (Median): Middle number of the (sorted) dataset : 50% = 2950\n",
    "    Third quartile: Middle number between median and maximum : 75% = 3312\n",
    "    Maximum: Highest number in the dataset : max = 4391\n",
    "\n",
    "It has been counted on 81 values and the mean Average Rental values is around 2798.\n",
    "\n",
    "A good news : most part of the neighborhoods (78 on a total of 81) are localised in areas with high walk scores (Walker’s Paradise or Very Walkable) and 53 of these 78 have the maximum of venues (50) allowing better chance to find a job.\n",
    "\n",
    "So we would concentrate on these areas to propose our adresses. In fact we would like to build a multi-dimentional table of areas where the best profile would be :\n",
    "- Highest Wakability scores\n",
    "- Highest number of Venues\n",
    "- Lowest Rental prices\n",
    "\n",
    "\n",
    "|Budget     |    Inf    |  Sup  |           \n",
    "|-----------|:---------:|:-----:|\n",
    "| *Cheap*   |     0     | 2149  |\n",
    "| *Average* |    2150   | 3149  |\n",
    "| *Pricey*  |    3150   |  '>'  |\n",
    "\n",
    "\n",
    "**Conclusion** \n",
    "\n",
    "- Most part of Neighborhoods are Walker’s Paradise  \n",
    "- Most part of Neighborhoods can offer maximum workplaces  (at least **50**)\n",
    "- But Neighborhoods are expansive : major part of rents are more than **2950** until **4391**\n",
    "- Finding a cheap rent is difficult and it costs beween **1625** until **2125**\n",
    "- And finally Average mean is quite high and lower than the Median rent : **2798** v.s. **2950**\n",
    "\n",
    "So it is why finding a job in the vicinity with a high walk score could allow to avoid spending money in cars.\n",
    "This is the next step of analysis.\n",
    "\n",
    "So the discrimination factor is clearly the price and the result of this study will be to provide the areas with the highest level of walkability, the highest density of venues and the lowest prices that could be resumed below :\n",
    "\n",
    "\n",
    "  | Budget    | Very Walkable | Walker’s Paradise | Prices            |\n",
    "  |-----------|:-------------:|:-----------------:|:-----------------:|\n",
    "  |Cheap\t  | \t  6 \t  | \t  13\t      | Y < 2150 \t      |\n",
    "  |Average\t  | \t  5 \t  |\t\t  30 \t\t  | 2149 < Y < 3150   |\n",
    "  |___________|_______________|___________________|___________________|\n",
    "  |Walk Score |   69 < X < 90 |\t\t X >= 90      |                   |\n",
    "\n",
    "\n",
    "\n",
    "Now let's study the venues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  e. Cartographic Localisation Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1. Target Neighborhoods in New York City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a map of New York City with  neighborhoods and venues superimposed on top.\n",
    "\n",
    " Also, let's show borders of NYC boroughs on our map and a few circles indicating scores and prices with colors indicating the category of the place rated by the Walk Scores and Pricing !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = 'New York City, NY'\n",
    "geo_agent = \"ny_explorer\"\n",
    "\n",
    "geolocator = Nominatim(user_agent=geo_agent)\n",
    "location = geolocator.geocode(address)\n",
    "latitude = location.latitude\n",
    "longitude = location.longitude\n",
    "print('The geograpical coordinate of {} are : {}, {}.'.format(address, latitude, longitude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neighborhood \t1BR Budget \tNeighborhood Latitude \tNeighborhood Longitude \tNumber Venues \tWS_mean WS_descr \tIndex\n",
    "# create map of New York using latitude and longitude values\n",
    "#df_ws_viz\n",
    "# add markers to map\n",
    "chartreuse = 'chartreuse'         # 'Walker’s Paradise'/cheap\n",
    "springgreen = 'springgreen'       # 'Very Walkable' /cheap\n",
    "olive = 'olive'                   # 'Somewhat Walkable' \n",
    "darkgreen = 'darkgreen'           # \n",
    "red = 'red'                       #  pricey \n",
    "blue = 'blue'                     # 'Walker’s Paradise'/pricey\n",
    "darkblue = 'darkblue'             # 'Very Walkable'/pricey\n",
    "forestgreen = 'forestgreen'       # 'Very Walkable'/average\n",
    "lightgreen = 'lightgreen'         # 'Walker’s Paradise'/ average\n",
    "dimgrey = 'dimgrey'\n",
    "gold ='gold'\n",
    "palegoldenrod = 'palegoldenrod'\n",
    "royalblue = 'royalblue'\n",
    "navy = 'navy'\n",
    "\n",
    "\n",
    "def get_nghb_color(ws, budget):\n",
    "    color = 'blue'\n",
    "    if (ws == 'Walker’s Paradise'):\n",
    "        if (budget == 'Cheap'):\n",
    "            color = chartreuse\n",
    "        elif (budget == 'Average'):\n",
    "            color = gold\n",
    "        else:\n",
    "            color = royalblue\n",
    "    elif (ws == 'Very Walkable'):\n",
    "        if (budget == 'Cheap'):\n",
    "            color = olive\n",
    "        elif (budget == 'Average'):\n",
    "            color = palegoldenrod\n",
    "        else:\n",
    "            color = navy\n",
    "    elif (ws == 'Somewhat Walkable'):\n",
    "            color = dimgrey\n",
    "    else: # car-dependant\n",
    "        color = red\n",
    "    return color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_newyork_v = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "for lat, lng, neighborhood,ws_descr, budget in zip(df_ws_viz['Neighborhood Latitude'], \n",
    "                                                   df_ws_viz['Neighborhood Longitude'],\n",
    "                                                   df_ws_viz['Neighborhood'],\n",
    "                                                   df_ws_viz['WS_descr'],\n",
    "                                                   df_ws_viz['Budget']\n",
    "                                                  ):\n",
    "    label = '{}-{},{}'.format(neighborhood, ws_descr, budget)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=5,\n",
    "        popup=label,\n",
    "        color=get_nghb_color(ws_descr, budget),\n",
    "        fill=True,\n",
    "        fill_color=get_nghb_color(ws_descr, budget), #'#3186cc',\n",
    "        fill_opacity=0.5,\n",
    "        parse_html=False).add_to(map_newyork_v)  \n",
    "    \n",
    "map_newyork_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Target Places in New York City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the same as above but visualizing venues locations with colors indicating the category of the place in function of the Walk Scores and Pricing !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import colors as mcolors\n",
    "#from folium.plugins import FloatImage\n",
    "\n",
    "#from folium.plugins import FloatImage\n",
    "#image_file = 'image.PNG'\n",
    "#FloatImage(image_file, bottom=0, left=86).add_to(mymap)\n",
    "map_newyork_v = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "#Neighborhood \tNeighborhood Latitude \tNeighborhood Longitude \t1BR \tBudget \tVenue \tAddress \tVenue Latitude \t\n",
    "# Venue Longitude \tVenue Category \tWalk_score \tWalk_descr            \n",
    "\n",
    "for index, row in df_venues_sorted.iterrows(): # venues_score_df.iterrows():\n",
    "#    if (index % 5 == 0):\n",
    "        name_n = row['Neighborhood']\n",
    "        name_v = row['Venue']\n",
    "        lat_n = row['Neighborhood Latitude']\n",
    "        lng_n = row['Neighborhood Longitude']\n",
    "        lat_v = row['Venue Latitude']\n",
    "        lng_v = row['Venue Longitude']\n",
    "        adr_v = row['Address']\n",
    "        ws    = row['Walk_descr']\n",
    "        budget= row['Budget']\n",
    "        br1   = row['1BR']\n",
    "        \n",
    "        color_v = get_nghb_color(ws, budget)\n",
    "        label = name_v + '\\n'+ adr_v + '\\n' + '[' + ws + '/' + budget + ':' + str(int(br1)) + '$]'\n",
    "\n",
    "        folium.CircleMarker([lat_v, lng_v], radius=3, color=color_v, fill=True,\n",
    "                            popup=label, fill_color='blue', fill_opacity=0.6).add_to(map_newyork_v)\n",
    "\n",
    "#map_newyork_v.get_root().html.add_child(folium.Element(legend_html)) \n",
    "map_newyork_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ws_viz_sorted =  df_ws_viz.copy().sort_values(['WS_mean', '1BR', 'Neighborhood Latitude', 'Neighborhood Longitude','Neighborhood'],\n",
    "                                                ascending=[False,True,True,True,True]).reset_index(drop=True)\n",
    "df_ws_viz_sorted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3. Heatmap of Places in Walk Scored Areas in New York City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a Choropleth heatmap of neighborhoods rated by Walk scores in New York City with  venues superimposed on top.\n",
    "\n",
    "with colors indicating the category of the places in relative to the Walk Scores !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_geo = r'new-york-city-boroughs.geojson' # geojson file\n",
    "NYC_loc = [40.7308619,-73.9871558]\n",
    "\n",
    "#df_ny = df_ws_viz.copy() #pd.read_csv('NYC_venues_walkability_0_3513.csv')\n",
    "# create a plain NYC map\n",
    "\n",
    "NYC_loc = [40.7308619,-73.9871558]\n",
    "ny_map = folium.Map(location=[40.7308619,-73.9871558], zoom_start=10, tiles='Mapbox Bright')\n",
    "ny_map.choropleth(\n",
    "    geo_data=ny_geo,\n",
    "    data=df_ws_viz_sorted,\n",
    "    columns=['Neighborhood', 'WS_mean'],\n",
    "    key_on='feature.properties.name',\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Walk scores neighborhood'\n",
    ")\n",
    "for index, row in df_venues_sorted.iterrows(): # venues_score_df.iterrows():\n",
    "    #if (index % 5 == 0):\n",
    "        name_n = row['Neighborhood']\n",
    "        name_v = row['Venue']\n",
    "        lat_n = row['Neighborhood Latitude']\n",
    "        lng_n = row['Neighborhood Longitude']\n",
    "        lat_v = row['Venue Latitude']\n",
    "        lng_v = row['Venue Longitude']\n",
    "        adr_v = row['Address']\n",
    "        ws    = row['Walk_descr']\n",
    "        budget= row['Budget']\n",
    "        br1   = row['1BR']\n",
    "        \n",
    "        color_v = get_nghb_color(ws, budget)\n",
    "        label = name_v + '\\n'+ adr_v + '\\n' + '[' + ws + '/' + budget + ':' + str(int(br1)) + '$]'\n",
    "\n",
    "        folium.CircleMarker([lat_v, lng_v], radius=3, color=color_v, fill=True,\n",
    "                            popup=label, fill_color='blue', fill_opacity=0.6).add_to(ny_map)\n",
    "# display map\n",
    "ny_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folium.__version__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  4. Heatmap of Places in Rental Price-based Rated Areas  in New York City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a heatmap of neighborhoods of New York City with  venues superimposed on top.\n",
    "\n",
    "with colors indicating the category of the places relative to Pricing !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load from local file system in case we did this before\n",
    "filename = 'NYC_venues_selected.csv'\n",
    "\n",
    "def load_selected_venues(df_venues):\n",
    "    filename = 'NYC_venues_selected.csv'\n",
    "    fromFile = False\n",
    "    try:\n",
    "        df_venues_sorted_limit = pd.read_csv(filename)\n",
    "        df_venues_sorted_limit.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "        # Reset order of categorical columns \n",
    "        df_venues_sorted['Walk_descr']= df_venues_sorted['Walk_score'].apply(lambda x: getWalkLabel(x))\n",
    "        df_venues_sorted.Walk_descr = set_col_cat(df_venues_sorted.Walk_descr, ws_cat)\n",
    "\n",
    "        df_venues_sorted.Budget = df_venues_sorted['1BR'].apply(lambda x: rescaleBudgetLabel(x))\n",
    "        df_venues_sorted.Budget = set_col_cat(df_venues_sorted.Budget)\n",
    "        print('NYC Neighborhoods Venues Selected loaded from file :', \n",
    "              filename)\n",
    "        fromFile = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # If not already loaded : use the Foursquare API to get the data\n",
    "    if not fromFile:\n",
    "        print (filename, ' : Data file for selected venues not existing ! Build it from df_venues')\n",
    "        df_venues_sorted = df_venues.copy()\n",
    "        df_venues_sorted['Walk_descr']= df_venues_sorted['Walk_score'].apply(lambda x: getWalkLabel(x))\n",
    "        df_venues_sorted.Walk_descr = set_col_cat(df_venues_sorted.Walk_descr, ws_cat)\n",
    "\n",
    "        df_venues_sorted.Budget = df_venues_sorted['1BR'].apply(lambda x: rescaleBudgetLabel(x))\n",
    "        df_venues_sorted.Budget = set_col_cat(df_venues_sorted.Budget)\n",
    "\n",
    "        #filter the wanted venues \n",
    "        df_venues_sorted_limit = df_venues_sorted[(df_venues_sorted['Walk_score'] >= 70)] # and (df_venues_sorted['1BR'] <= 3150)]\n",
    "        df_venues_sorted_limit = df_venues_sorted_limit[df_venues_sorted_limit['1BR'] < 3150]\n",
    "\n",
    "        print ('df_venues_sorted_limit shape ', df_venues_sorted_limit.shape)\n",
    "        # Sort according our criteria Walk_decr and not Walk_score because \n",
    "        df_venues_sorted_limit = df_venues_sorted_limit.sort_values(['Walk_descr','1BR', 'Walk_score','Budget'],\n",
    "                                                                      ascending=[False,True,False,True]).reset_index(drop=True)\n",
    "        # Export the dataframe values to a local file  \n",
    "        #print(df)\n",
    "        df_venues_sorted_limit.to_csv(filename)\n",
    "        print ('Selected Venues Data file created : ', filename, '-> shape = ', df_venues_sorted_limit.shape)\n",
    "    return df_venues_sorted_limit\n",
    "\n",
    "df_venues_sorted_limit = load_selected_venues(df_venues)\n",
    "# New dataframe of selected venues in the correct order is available to be analysed and visualized \n",
    "# with normally in saved data file : \n",
    "# Selected Venues Data file created :  NYC_venues_selected.csv -> shape =  (1825, 12)\n",
    "df_venues_sorted_limit.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ny_geo = r'new-york-city-boroughs.geojson' # geojson file\n",
    "\n",
    "#df_ny = df_ws_viz.copy() #pd.read_csv('NYC_venues_walkability_0_3513.csv')\n",
    "# create a plain NYC map\n",
    "\n",
    "NYC_loc = [40.7308619,-73.9871558]\n",
    "ny_map_p = folium.Map(location=[40.7308619,-73.9871558], zoom_start=10, tiles='Mapbox Bright')\n",
    "ny_map_p.choropleth(\n",
    "    geo_data=ny_geo,\n",
    "    data=df_ws_viz_sorted,\n",
    "    columns=['Neighborhood', '1BR'], #['Neighborhood Latitude','Neighborhood Longitude','1BR','Walk_score'],\n",
    "    key_on='feature.properties.name',\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Venues Prices Scored'\n",
    ")\n",
    "for index, row in df_venues_sorted.iterrows(): # venues_score_df.iterrows():\n",
    "#    if (index % 5 == 0):\n",
    "        name_n = row['Neighborhood']\n",
    "        name_v = row['Venue']\n",
    "        lat_n = row['Neighborhood Latitude']\n",
    "        lng_n = row['Neighborhood Longitude']\n",
    "        lat_v = row['Venue Latitude']\n",
    "        lng_v = row['Venue Longitude']\n",
    "        adr_v = row['Address']\n",
    "        ws    = row['Walk_descr']\n",
    "        budget= row['Budget']\n",
    "        br1   = row['1BR']\n",
    "        \n",
    "        color_v = get_nghb_color(ws, budget)\n",
    "        label = name_v + '\\n'+ adr_v + '\\n' + '[' + ws + '/' + budget + ':' + str(int(br1)) + '$]'\n",
    "\n",
    "        folium.CircleMarker([lat_v, lng_v], radius=3, color=color_v, fill=True,\n",
    "                            popup=label, fill_color='blue', fill_opacity=0.6).add_to(ny_map_p)\n",
    "# display map\n",
    "ny_map_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok That looks good. Now we have about 2 thousands of venues (1825) localized in New York City and a good visualization of the places rated according to their Walkability or observe rental Pricing of 1 Bedroom apartment.\n",
    "\n",
    "We can graphically say that several sections appear like (but not the only ones) :\n",
    "- Good candidate places where rental price is 'cheap' and can live and work without owning a car \n",
    "    - Justice Ave (btwn Broadway & 52nd Ave), Elmhurst, NY 11373 (*Walker’s Paradise/Cheap 1900)\n",
    "    - Woodhaven, NY 11421 (*Walker’s Paradise/Cheap:1625)\n",
    "    - Ave (Jamaica Av), Richmond Hill, NY 11418 (*Walker’s Paradise/Cheap:1663)\n",
    "    - Lefferts Blvd, South Ozone Park, NY 11420 (*Very Walkable/Cheap:1663)\n",
    "    - etc...\n",
    "\n",
    "but :\n",
    "- Places to avoid if you have limited financial means even if walkable\n",
    "    - Ave (at Metropolitan Ave), Brooklyn, NY 11211 (Walker’s Paradise/Pricey:3300)\n",
    "    - Flatiron District New York, NY 10010 (Walker’s Paradise/Pricey:3552)\n",
    "    -  Grand Ave, Maspeth, NY 11378 (Car-Dependant/Average:2331)\n",
    "    - (Bld. 292), Brooklyn, NY 11205 (Car-Dependant/Pricey:3312)\n",
    "    - etc...\n",
    "\n",
    "And could continue along the list of adresses provided thanks to Foursquare for the venues, Walkscore for the pedestrian well-being and Renthop for the rental pricing.\n",
    "\n",
    "Let us now cluster those locations to create a list of areas around venues gathering all our conditions i.e hihgly walkable with low rental prices and containing the maximum potential workplaces. The list issued from the processing ids the one that can be provided to stakeholders and usefull for people looking for places to live for cheap and with a good quality of life. \n",
    "\n",
    "Those zones, their centers and addresses will be the final result of our analysis. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's perform now the last phase consisting in building the proposal to the stakeholders defining a set of neighborhoods and places in New York City combining the criteria we set up in the proble statement : place with no car-dependance, with high potential to find work places  and with the lowest rental prices.\n",
    "\n",
    "This will lead us to define the clusters matching our criteria :\n",
    "- First : at neighborhood level to have a high level map of areas of interest\n",
    "- Second : at venues level to define areas adresses for a more accurate map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve and prepare the raw data needed for clustering matching the Walk Score, Number of venues and Rental Prices : \n",
    "\n",
    "- for neighborhoods \n",
    "- for venues "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1. Target Neighborhoods  Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd_ngh = pd.DataFrame([], index=budget_cat, columns=ws_cat)\n",
    "dd = df_info_viz[['Budget', 'WS_descr', 'Number Venues']].copy()\n",
    "dfd_ngh.apply(calc_total, args =('Count',dd), axis=1)\n",
    "\n",
    "df_nac_summary =  dfd_ngh.loc[['Average','Cheap'],['Very Walkable','Walker’s Paradise']]\n",
    "total_n = dfd_ngh.astype(float).sum().sum().astype(int)\n",
    "print ('Total Neighborhoods = ', total_n)\n",
    "total_ac_vp = df_nac_summary.astype(float).sum().astype(int)\n",
    "\n",
    "print ('Total Neighborhoods = ', total_n, 'Neighborhoods fitting the criteria = ',total_ac_vp)\n",
    "\n",
    "print('Total number of neighborhoods with Average or Cheap Rentals and more than Very Walkable = ', total_ac_vp.sum(),\n",
    "      '\\n -> Representing a rate of : ', str(round(total_ac_vp.sum()/total_n * 100).astype(int))+'%', )\n",
    "\n",
    "df_nac_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are our neighborhoods set fitting the requirements distributed in 4 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build target neighborhoods raw data\n",
    "#print(df_info_viz.dtypes, '\\n', df_info_viz.shape)\n",
    "\n",
    "df_neigh = df_info_viz.copy().reset_index(drop=True)\n",
    "df_neigh = df_info_viz.copy()[df_info_viz['WS_descr'] >= 'Very Walkable']\n",
    "df_neigh = df_neigh[df_neigh['Budget'] <= 'Average'] \n",
    "\n",
    "#print(df.dtypes, '\\n', df.shape)\n",
    "df_neigh = df_neigh.sort_values(['WS_mean','1BR', 'Number Venues'],ascending=[False,True,False]).reset_index(drop=True)\n",
    "df_neigh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we kept only our 45 over 81 of all neighborhoods prices are relatively affordable and with high Walkability.\n",
    "\n",
    "Let's repeat the same calculus to explore now the number of venues for potential workplaces relative to Walkability scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Targeted Venues Data Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd_nv = pd.DataFrame([], index=budget_cat, columns=ws_cat)\n",
    "\n",
    "dfd_nv.apply(calc_total, args=('Number Venues', df_info_viz[['Budget', 'WS_descr', 'Number Venues']].copy()), axis=1)\n",
    "df_ac =  dfd_nv.loc[['Average','Cheap'],['Very Walkable','Walker’s Paradise']]\n",
    "\n",
    "total_v= dfd_nv.loc[['Cheap','Average','Pricey'],['Very Walkable','Walker’s Paradise']].astype(float).sum().sum().astype(int)\n",
    "print ('Total Venues Very Walkable or Walker\\'s Paradise = ', total_v)\n",
    "total_ac_vp = df_ac.astype(float).sum().astype(int)\n",
    "print('Total number of venues with lowest Rentals and more than Very Walkable = ', total_ac_vp.sum(),\n",
    "      '\\n -> Representing a rate of : ', str(round(total_ac_vp.sum()/total_v * 100).astype(int))+'%')\n",
    "\n",
    "dfd_nv = dfd_nv.loc[['Average','Cheap'],['Very Walkable','Walker’s Paradise']]\n",
    "dfd_nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's re-build the venues fitting the requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are our venues set fitting the requirements distributed also as the neighborhoods in 4 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try to load from local file system in case we did this before\n",
    "filename = 'NYC_venues_selected.csv'\n",
    "\n",
    "def load_selected_venues(df_venues):\n",
    "    filename = 'NYC_venues_selected.csv'\n",
    "    fromFile = False\n",
    "    try:\n",
    "        df_venues_sorted_limit = pd.read_csv(filename)\n",
    "        df_venues_sorted_limit.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "        # Reset order of categorical columns \n",
    "        df_venues_sorted['Walk_descr']= df_venues_sorted['Walk_score'].apply(lambda x: getWalkLabel(x))\n",
    "        df_venues_sorted.Walk_descr = set_col_cat(df_venues_sorted.Walk_descr, ws_cat)\n",
    "\n",
    "        df_venues_sorted.Budget = df_venues_sorted['1BR'].apply(lambda x: rescaleBudgetLabel(x))\n",
    "        df_venues_sorted.Budget = set_col_cat(df_venues_sorted.Budget)\n",
    "        print('NYC Neighborhoods Venues Selected loaded from file :', \n",
    "              filename)\n",
    "        fromFile = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # If not already loaded : use the Foursquare API to get the data\n",
    "    if not fromFile:\n",
    "        print (filename, ' : Data file for selected venues not existing ! Build it from df_venues')\n",
    "        df_venues_sorted = df_venues.copy()\n",
    "        df_venues_sorted['Walk_descr']= df_venues_sorted['Walk_score'].apply(lambda x: getWalkLabel(x))\n",
    "        df_venues_sorted.Walk_descr = set_col_cat(df_venues_sorted.Walk_descr, ws_cat)\n",
    "\n",
    "        df_venues_sorted.Budget = df_venues_sorted['1BR'].apply(lambda x: rescaleBudgetLabel(x))\n",
    "        df_venues_sorted.Budget = set_col_cat(df_venues_sorted.Budget)\n",
    "\n",
    "        #filter the wanted venues \n",
    "        df_venues_sorted_limit = df_venues_sorted[(df_venues_sorted['Walk_score'] >= 70)] # and (df_venues_sorted['1BR'] <= 3150)]\n",
    "        df_venues_sorted_limit = df_venues_sorted_limit[df_venues_sorted_limit['1BR'] < 3150]\n",
    "\n",
    "        print ('df_venues_sorted_limit shape ', df_venues_sorted_limit.shape)\n",
    "        # Sort according our criteria Walk_decr and not Walk_score because \n",
    "        df_venues_sorted_limit = df_venues_sorted_limit.sort_values(['Walk_descr','1BR', 'Walk_score','Budget'],\n",
    "                                                                      ascending=[False,True,False,True]).reset_index(drop=True)\n",
    "        # Export the dataframe values to a local file  \n",
    "        #print(df)\n",
    "        df_venues_sorted_limit.to_csv(filename)\n",
    "        print ('Selected Venues Data file created : ', filename, '-> shape = ', df_venues_sorted_limit.shape)\n",
    "    return df_venues_sorted_limit\n",
    "\n",
    "df_venues_sorted_limit = load_selected_venues(df_venues)\n",
    "# New dataframe of selected venues in the correct order is available to be analysed and visualized \n",
    "# with normally in saved data file : \n",
    "# Selected Venues Data file created :  NYC_venues_selected.csv -> shape =  (1825, 12)\n",
    "df_venues_sorted_limit.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print ('df_venues_sorted_limit shape = ', df_venues_sorted_limit.shape)\n",
    "df_vslc = df_venues_sorted_limit.groupby('Neighborhood',as_index=False).count().sort_values('Venue',ascending=True).reset_index(drop=True)\n",
    "df_vslc['Number Venues'] = df_vslc['Venue']\n",
    "print ('df_vslc grouped by venues number shape = ', df_vslc.shape)\n",
    "df_vslc = df_vslc[['Neighborhood', 'Number Venues']]\n",
    "df_vslc = df_vslc.sort_values('Number Venues',ascending=False).reset_index(drop=True)\n",
    "df_vslc.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2. Clustering of Neighborhoods in New York City"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  a. Pre-processing\n",
    "\n",
    "Before clustering we have to preprocess the data :\n",
    "- Removing useless columns (categorical, string etc...)\n",
    "- Normalizing data to be able to interpret features with different magnitudes and distributions equally... We will StandardScaler() to normalize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_neigh.copy()\n",
    "\n",
    "# Transform categorical in value number (rank)\n",
    "#ws_cat = ['Car-Dependant', 'Somewhat Walkable', 'Very Walkable', 'Walker’s Paradise']\n",
    "#budget_cat =['Cheap', 'Average', 'Pricey']\n",
    "# Set 0 for lower values : 'Average' and 'Very Walkable', 1 otherwise\n",
    "df['Budget_num'] = df['Budget'].apply(lambda x: convertCatToNum(x,'b')).astype(int)\n",
    "df['WS_num'] = df['WS_descr'].apply(lambda x: convertCatToNum(x,'w')).astype(int)\n",
    "\n",
    "# Now Removing useless columns : all non numerical\n",
    "todrop = ['Neighborhood','Neighborhood Latitude','Neighborhood Longitude', 'Budget', 'WS_descr', 'Index']\n",
    "for col in todrop:\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "print(df.dtypes,'   -> ', df.shape)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing over the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.values[:,1:]\n",
    "X = np.nan_to_num(X)\n",
    "df_norm = StandardScaler().fit_transform(X)\n",
    "print(X.shape, df_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling Neighborhood Clusters with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Should present 4 clusters\n",
    "clusterNum = 4\n",
    "k_means_ngh = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12)\n",
    "k_means_ngh.fit(X)\n",
    "labels = k_means_ngh.labels_\n",
    "print(len(labels), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[\"Clus_lbl\"] = labels\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_neigh[\"Clus_lbl\"] = labels\n",
    "df_neigh.sort_values(['Clus_lbl','WS_mean', '1BR', 'Number Venues'],ascending=[True, False,True, False]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems that we dhave not exactly what we have found when manually sorting the dataset.\n",
    "Except for Cluster 1 that seems rather close to previous results (same number as Average x Walker's Paradise). But we can see some Cheap 1BR Budget categorized inside this one.\n",
    "That means that we need to tweak a little more the parameters.\n",
    "\n",
    "I think the reason is the weight of each parameter : here all parameters have the same weight.\n",
    "Likewise, we have also replaced the categorical attributes 'Budget' and 'WS_descr' by 0 or 1. \n",
    "\n",
    "Perhaps we should found most appropriate values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neighborhood Clusters Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Clus_lbl').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Clus_lbl').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map of  Neighborhoods Localisation by Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_newyork_cv = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
    "\n",
    "for lat, lng, neighborhood,ws_descr, budget, br1 in zip(df_neigh['Neighborhood Latitude'], \n",
    "                                                   df_neigh['Neighborhood Longitude'],\n",
    "                                                   df_neigh['Neighborhood'],\n",
    "                                                   df_neigh['WS_descr'],\n",
    "                                                   df_neigh['Budget'],\n",
    "                                                   df_neigh['1BR']\n",
    "                                                  ):\n",
    "    label = '{}-{},{}:${} '.format(neighborhood, ws_descr, budget, br1)\n",
    "    label = folium.Popup(label, parse_html=True)\n",
    "    folium.CircleMarker(\n",
    "        [lat, lng],\n",
    "        radius=10,\n",
    "        popup=label,\n",
    "        color=get_nghb_color(ws_descr, budget),\n",
    "        fill=True,\n",
    "        fill_color='#3186cc', #get_nghb_color(ws_descr, budget), #'#3186cc',\n",
    "        fill_opacity=0.5,\n",
    "        parse_html=False).add_to(map_newyork_cv)  \n",
    "    \n",
    "map_newyork_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Clustering of Target Places by Venues in New York City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last step we will repeat the same processing as performed above but this time this will relate to the venues.\n",
    "\n",
    "Like above, we repeat the same steps before clustering. We have to preprocess the data :\n",
    "- Removing useless columns (categorical, string etc...)\n",
    "- Normalizing data to be able to interpret features with different magnitudes and distributions equally... We will StandardScaler() to normalize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_v = df_venues_sorted_limit.copy()\n",
    "#print(df_v.shape, df_v.dtypes)\n",
    "df_v.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform categorical in value number (rank)\n",
    "df_venues = df_v.copy()\n",
    "\n",
    "#ws_cat = ['Car-Dependant', 'Somewhat Walkable', 'Very Walkable', 'Walker’s Paradise']\n",
    "#budget_cat =['Cheap', 'Average', 'Pricey']\n",
    "# Set 0 for lower values : 'Average' and 'Very Walkable', 1 otherwise\n",
    "df_v['Budget_num'] = df_v['Budget'].apply(lambda x: convertCatToNum(x,'b')).astype(int)\n",
    "df_v['WS_num'] = df_v['Walk_descr'].apply(lambda x: convertCatToNum(x,'w')).astype(int)\n",
    "\n",
    "# Now Removing useless columns : all non numerical\n",
    "# Neighborhood, Neighborhood Latitude, Neighborhood Longitude,Budget,Venue,Address,\n",
    "# Venue Latitude \tVenue Longitude, Venue Category, Walk_descr\n",
    "todrop = ['Neighborhood','Neighborhood Latitude','Neighborhood Longitude', 'Budget',\n",
    "          'Venue','Address','Venue Latitude','Venue Longitude','Venue Category',\n",
    "          'Walk_descr']\n",
    "\n",
    "for col in todrop:\n",
    "    df_v.drop(col, axis=1, inplace=True)\n",
    "\n",
    "print(df_v.dtypes,'   -> ', df_v.shape)\n",
    "df_v.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing over the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_v = df_v.values[:,1:]\n",
    "X_v = np.nan_to_num(X_v)\n",
    "df_norm_v = StandardScaler().fit_transform(X_v)\n",
    "print(X_v.shape, df_norm_v.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling Neighborhood Clusters with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Should present 4 clusters\n",
    "clusterNum = 4\n",
    "k_means_v = KMeans(init = \"k-means++\", n_clusters = 4, n_init = 12)\n",
    "k_means_v.fit(X_v)\n",
    "labels_v = k_means_v.labels_\n",
    "print(len(labels_v), labels_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_venues['Clus_lbl'] = labels_v\n",
    "df_venues.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Neighborhood Clusters Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venues.groupby('Clus_lbl').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_venues.groupby('Clus_lbl').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, it seems that we have the same behaviour observed for the neighborhood. Well same causes same consequences !\n",
    "\n",
    "The results are not exactly what we have found when manually sorting the dataset.\n",
    "\n",
    "That means that we really need to tweak more the parameters.\n",
    "\n",
    "Perhaps we have to found most appropriate values. This should be done in a future work of course !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Heatmap of neighborhoods clusters rated by Walk scores in New York City  with  venues superimposed on top.\n",
    "\n",
    "And colors indicating the category of the places in relative to the Walk Scores !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_geo = r'new-york-city-boroughs.geojson' # geojson file\n",
    "# create a plain NYC map\n",
    "\n",
    "NYC_loc = [40.7308619,-73.9871558]\n",
    "ny_map_v = folium.Map(location=[40.7308619,-73.9871558], zoom_start=10, tiles='Mapbox Bright')\n",
    "ny_map_v.choropleth(\n",
    "    geo_data=ny_geo,\n",
    "    data=df_ws_viz_sorted,\n",
    "    columns=['Neighborhood', 'WS_mean'], #['Neighborhood Latitude','Neighborhood Longitude','1BR','Walk_score'],\n",
    "    key_on='feature.properties.name',\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Walk Score Neighborhood Scale'\n",
    ")\n",
    "for index, row in df_venues.iterrows(): # venues_score_df.iterrows():\n",
    "        name_n = row['Neighborhood']\n",
    "        name_v = row['Venue']\n",
    "        lat_n = row['Neighborhood Latitude']\n",
    "        lng_n = row['Neighborhood Longitude']\n",
    "        lat_v = row['Venue Latitude']\n",
    "        lng_v = row['Venue Longitude']\n",
    "        adr_v = row['Address']\n",
    "        ws    = row['Walk_descr']\n",
    "        budget= row['Budget']\n",
    "        br1   = row['1BR']\n",
    "        \n",
    "        color_v = get_nghb_color(ws, budget)\n",
    "        label = name_v + '\\n'+ adr_v + '\\n' + '[' + ws + '/' + budget + ':' + str(int(br1)) + '$]'\n",
    "\n",
    "        folium.CircleMarker([lat_v, lng_v], radius=3, color=color_v, fill=True,\n",
    "                            popup=label, fill_color='blue', fill_opacity=0.6).add_to(ny_map_v)\n",
    "# display map\n",
    "ny_map_v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We have defined a visual tool to allow people to easily search for an apartment to rent according to the criteria they want, based on :\n",
    " - pedestrian possibilities or walkability of a place within a radius of 500 m from the center of visualized circle\n",
    " - the opportunity degree to find a job and \n",
    " - the amount of rent they want to dedicate. \n",
    " \n",
    "These maps of neighborhoods and places can ease the life of the stakeholders and answer to the question we defined at the problenm statement.\n",
    "\n",
    "Of course this is a first draft that needs improvement to be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data available for a non professionnal project, so limited in number and quality, our analysis shows that we were able to retrieve several thousands of basic information on places (a number of about 3500 venues in New York) represented about only 80 neighborhoods over the 306 existing in New York City.\n",
    "\n",
    "So, unfortunately these data does not cover the totality of New City.\n",
    "\n",
    "The most part of the work performed here was to collect and process the data so that it could be used for analysis.\n",
    "\n",
    "Based on this data set, we can extract the points of interest gathering all the criteria defined in the requirements exposed in the problem statement.\n",
    "\n",
    "We filtered out the elements that constitute a good starting point to consider the areas fitting the following criteria :\n",
    "\n",
    "- pedestrian possibilities or walkability of a place within a radius of 500 m from the center of visualized circle\n",
    "- opportunity degree to find a job and\n",
    "- needed amount of money to rent a 1 bedroom apartment to locate they want to dedicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally succeded in partitioning manually the candidate places in 4 categories.\n",
    "\n",
    "We tried to reproduce this categorisation using clustering methods. The obtained cluters do not fit exactly those obtained manually.\n",
    "\n",
    "The clustering parameters should be tweaked more carefully. In particular, it should be possible to try and adjust to reflect the importance given to each criterion which can change the nature and composition of the clusters.\n",
    "\n",
    "But it is a promising track. \n",
    "\n",
    "We could already propose a list of adresses of places, summarized on a interactive maps of New York City that would meet the needs of customers.\n",
    "\n",
    "This work could interest directly categories of professionnals like online rental agencies or recommendation tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *Applied Data Science Capstone*. If you accessed this notebook outside the course, you can take this course online by clicking [here](http://cocl.us/DP0701EN_Coursera_Week3_LAB2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2018 [Cognitive Class](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
